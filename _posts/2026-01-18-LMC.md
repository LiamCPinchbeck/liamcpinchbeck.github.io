---
title: "Speeding up MCMC with Langevin and Hamiltonian dynamics and stochastic gradient estimates"
date: 2026-01-18
permalink: /posts/2026/01/2026-01-18-LMC/
tags:
  - MCMC
  - Langevin Monte Carlo
  - Stochastic Processes
header-includes:
  - \usepackage{amsmath}
  - \usepackage{algpseudocode}

---

In this post I'm going to try and introduce Hamiltonian and Langevin Monte Carlo, and there stochastic gradient counterparts (among a couple other things). This post will involve a little stochastic differential calculus and some results from [A Complete Recipe for Stochastic Gradient MCMC - Ma et al. (2015)](https://arxiv.org/abs/1506.04696). (UNDER CONSTRUCTION)


# UNDER CONSTRUCTION UNDER CONSTRUCTION UNDER CONSTRUCTION UNDER CONSTRUCTION UNDER CONSTRUCTION UNDER CONSTRUCTION


---

## Resources

### Main Resources

- [A Complete Recipe for Stochastic Gradient MCMC - Ma et al. (2015)](https://arxiv.org/abs/1506.04696)
- [Microcanonical Hamiltonian Monte Carlo](https://arxiv.org/abs/2212.08549)
- [Fluctuation without dissipation: Microcanonical Langevin Monte Carlo](https://arxiv.org/abs/2303.18221)


### LMC/Langevin Methods

- [Exponential convergence of Langevin distributions and their discrete approximations - Gareth O. Roberts, Richard L. Tweedie](https://projecteuclid.org/journals/bernoulli/volume-2/issue-4/Exponential-convergence-of-Langevin-distributions-and-their-discrete-approximations/bj/1178291835.full)
- [Brownian dynamics as smart Monte Carlo simulation - P. J. Rossky; J. D. Doll; H. L. Friedman](https://pubs.aip.org/aip/jcp/article-abstract/69/10/4628/595731/Brownian-dynamics-as-smart-Monte-Carlo-simulation?redirectedFrom=fulltext)
- [Bayesian Learning via Stochastic Gradient Langevin Dynamics - Max Welling, Yee Whye Teh](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf)
- [Metropolis-adjusted Monte Carlo - Wiki](https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm)
- [Towards a Theory of Non-Log-Concave Sampling: First-Order Stationarity Guarantees for Langevin Monte Carlo - Krishnakumar Balasubramanian, Sinho Chewi, Murat A. Erdogdu, Adil Salim, Matthew Zhang](https://arxiv.org/abs/2202.05214)
- [Efficient Approximate Posterior Sampling with Annealed Langevin Monte Carlo - Advait Parulekar, Litu Rout, Karthikeyan Shanmugam, and Sanjay Shakkottai](https://arxiv.org/abs/2508.07631)
- [Langevin Monte Carlo: random coordinate descent and variance reduction - Zhiyan Ding, Qin Li](https://arxiv.org/abs/2007.14209)
- [Random Reshuffling for Stochastic Gradient Langevin Dynamics - Luke Shaw, Peter A. Whalley](https://arxiv.org/abs/2501.16055)
- [Analysis of Langevin Monte Carlo via convex optimization - Alain Durmus, Szymon Majewski, and Blazej Miasojedow](https://arxiv.org/abs/1802.09188)
- [A Divergence Bound For Hybrids of MCMC and Variational Inference and ](https://justindomke.wordpress.com/2017/11/16/a-divergence-bound-for-hybrids-of-mcmc-and-variational-inference-and/)
- [Generative Modeling by Estimating Gradients of the Data Distribution - Yang Song](https://yang-song.net/blog/2021/score/)
- [Sampling as optimization in the space of measures ... - COLT](https://www.youtube.com/watch?v=v6yL4t780KY)
- [Random Coordinate Descent and Langevin Monte Carlo - Qin Li, Simons Institute for the Theory of Computing](https://youtu.be/TaXvdb7HCas)
- [The Sampling Problem Through The Lens of Optimization : Recent Advances and Insights by Aniket Das](https://youtu.be/ufDU59FSCls)



### For Langevin Dynamics

- [A Simplified Overview of Langevin Dynamics - Roy Friedman](https://friedmanroy.github.io/blog/2022/Langevin/)
- [On the Probability Flow ODE of Langevin Dynamics - Mingxuan Yi](https://mingxuan-yi.github.io/blog/2023/prob-flow-ode/)
- [Langevin Equation - Wiki](https://en.wikipedia.org/wiki/Langevin_equation)
- [\[DeepBayes2019\]: Day 5, Lecture 3. Langevin dynamics for sampling and global optimization - Kirill Neklyudov](https://youtu.be/3-KzIjoFJy4)
    - Great derivation for the Fokker-Planck equation and a few other results (that I was heavily 'inspired' by in my post).

### For HMC

- [Microcanonical Hamiltonian Monte Carlo](https://arxiv.org/abs/2212.08549)
- [Stochastic Gradient Hamiltonian Monte Carlo](https://arxiv.org/abs/1402.4102)
- [A Conceptual Introduction to Hamiltonian Monte Carlo - Betancourt](https://arxiv.org/abs/1701.02434)
    - Pretty good imo but it's 60 pages. I found the appendices to be pretty helpful with the NUTS implementation of HMC (specifically A.4)

### Misc

- [A prequel to the diffusion model - Nosuke Dev Blog](https://nosuke.dev/posts/denoising_and_score/)
- [Santa Jaws](https://www.imdb.com/title/tt8305692/)
- [Improving Diffusion Models as an Alternative To GANs, Part 2 - NVIDIA Technical Blog](https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-2/)
- [Continuous Algorithms: Sampling and Optimization in High Dimension - Santesh Vempala, Simons Institute for the Theory of Computing](https://youtu.be/q1uq0kMsX-Y)
- [Non-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic analysis](https://arxiv.org/abs/1702.03849)
- [Handbook of Stochastic Methods - Gardiner](https://d-nb.info/948236752/04)
- [Stochastic Differential Equations - An Introduction with Applications - Bernt ksendal](http://www.stat.ucla.edu/~ywu/research/documents/StochasticDifferentialEquations.pdf)
- [An Introduction to Stochastic Differential Equations - Lawrence C. Evans](https://www.amazon.com.au/dp/1470410540)
    - This is a particularly great reference for those not comfortable with measure theory interpretations of probability
    - And ended up being what I structured my background sections on SDE on (skipping a lot of details and rigor)
- [Probability Theory in Finance: A Mathematical Guide to the Black-Scholes Formula (Graduate Studies in Mathematics) - Sean Dineen](https://www.amazon.com.au/Probability-Theory-Finance-Mathematical-Black-Scholes/dp/0821839519)
    - Much more informal-style or conversational take on the above topics more catered for finance but is great as a reference for some more abstractly presented topics in Evans
    - Also I think there is more use of examples around novel propositions and definitions to help solidify said abstract ideas but can be seen as fluff if you aren't interested/find the given ideas simple






---

## Table of Contents


1. [The Motivation: Why "Random" Isn't Enough](#the-motivation-why-random-isnt-enough)
2. [A sneak peak: A purely intuitive idea of our goal](#a-sneak-peak-at-the-goal-of-the-post-an-intuitive-picture-of-langevin-monte-carlo)
3. [Stochastic Calculus (and a lil' Physics?)](#stochastic-calculus-and-a-lil-physics)
    - [Introduction to Stochastic Differential Equations (SDEs)](#introduction-to-stochastic-differential-equations-sdes)
    - [Stochastic Integrals: It么 Calculus vs. Stratonovich](#stochastic-integrals-it么-calculus-vs-stratonovich)
    - [The Langevin SDE](#the-langevin-sde-combining-deterministic-drift-and-random-fluctuations)
    - [The Fokker-Planck Equation](#the-fokker-planck-equation-flowing-probability)
    - [Stationary Solutions to the SDE](#stationary-solutions-to-the-sde)
4. [Hamiltonian MCMC and NUTS in detail](#hamiltonian-mcmc-and-nuts-in-detail)
5. [LMC in detail](#lmc-in-detail)
5. [Recipes for Stochastic Gradient MCMC](#recipes-for-stochastic-gradient-mcmc)
6. [Exploring Methods discussed in Ma et al. (2015)](#exploring-methods-discussed-in-ma-et-al-2015)

---

# The Motivation: Why "Random" Isn't Enough

MCMC is one of the most revolutionary and widely used tools in a statisticians arsenal when it comes to exploring probability distributions. 
But despite this, it is deceptively simple and possibly doesn't use all available information to speed up sampling.
In this post we're going to see some ways that we can possibly use some of this available information to do exactly that.

First, I'll do a quick refresher for what Metropolis-Hasting MCMC is. If you are completely, unfamiliar with Metropolis-Hasting MCMC I'd recommend (completely unbiased) [my practical guide on MCMC and especially the resources/references within](/posts/2025/01/2025-01-29-practical-MHA-intro/) but I'll cover the main points below.


Metropolis-Hasting MCMC (MHMCMC) comprises of three things: a (typically unnormalised) target distribution, a _proposal_ distribution and how many iterations of the below algorithm you can bare to wait for. 

>
#### Metropolis-Hastings Algorithm
1. Initialise: 
    1. Have a distribution you want to sample from (duh) $$f(x)$$,
    2. manually create a starting point for the algorithm $$x_0$$,
    3. pick a distribution $$g(x\mid y)$$ to sample from 
    4. pick the number of samples you can be bothered waiting for $$N$$
2. For each iteration $$n$$/Repeat $$N$$ times
    1. Sample a new _proposal point_ $$x^*$$ from the symmetric distribution centred at the previous sample 
    2. Calculate the _acceptance probability_ $$\alpha$$ given as $$\alpha = \frac{f(x^*)g(x_n\mid x^*)}{f(x_n)g(x^*\mid x_n)}$$ <small>(Here's the change!)</small>
        - And if the acceptance probability is more than 1, cap it at 1.
    3. Generate a number, $$u$$, from a uniform distribution between 0 and 1
    4. Accept or reject
        1. Accept if: $$u\leq\alpha$$, s.t. $$x_{n+1} = x^*$$
        2. Reject if: $$u>\alpha$$, s.t. $$x_{n+1} = x_n$$


This amounts to exploring a given distribution in the method indicated in the GIF below.

<div style="text-align: center;">
<img 
    src="/files/BlogPostData/2025-01-29/metropolis_gif.gif" 
    alt="GIF showing the process of an MHMCMC Algorithm (with symmetric proposal making it equivalent to Metropolis MCMC)" 
    title="GIF showing the process of an MHMCMC Algorithm (with symmetric proposal making it equivalent to Metropolis MCMC)" 
    style="width: 49%; height: auto; border-radius: 1px;">
<img 
    src="/files/BlogPostData/2025-01-29/metropolis_slow_gif.gif" 
    alt="GIF showing the process of an MHMCMC Algorithm (with symmetric proposal making it equivalent to Metropolis MCMC)" 
    title="GIF showing the process of an MHMCMC Algorithm (with symmetric proposal making it equivalent to Metropolis MCMC)" 
    style="width: 49%; height: auto; border-radius: 1px;">
</div>
<br>

The target distribution is the distribution that we are trying to generate samples for and all we need is a way to evaluate it up to a normalisation constant. The proposal distribution is then some other distribution, that uses the previous accepted value that we have sampled, to propose a new point (and either accept-reject based on the density ratio with respect to the previous accepted sample). This amounts to something we call a Markov Process, which can be viewed as a discrete-time [_stochastic_ or _random_ process](https://en.wikipedia.org/wiki/Stochastic_process) (we will revisit this later), where every sample/event only depends on the previous. 

This is why Metropolis MCMC is sometimes referred to as a type of ["Gaussian random walk"](https://en.wikipedia.org/wiki/Gaussian_random_walk) (in the case of a gaussian proposal distribution) or Random Walk Metropolis (RWM, as I will more generally refer to for the above types of MCMC algorithms).

These algorithms have been stupidly successful, being used from investigating the properties of stars in other galaxies to how the share prices of toy companies will vary in a given month. The key benefit that came from it being the ability to partially side-step the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) that arises if one were to naively grid every dimension of a given distribution one wanted to investigated.

RWM works great for moderately dimensional spaces (very roughly from my experience around 15 dimensions for non-pathological distributions) but is not able to get around the curse entirely of course.

There are two key issues that RWM struggles with in higher dimensional spaces: the way volume behaves in these spaces, and the general _in_-efficiency of the process as a function of dimension.

## Pathological Volumes and Shells

In high dimensional spaces, very basic things one would presume about volume and space break down. e.g. A particularly fun thing that can happen regarding spheres escaping the boxes that you would think contain them is explained [in this numberphile video](https://youtu.be/mceaM2_zQd8). 

The particular issue in the case of RWM algorithms is that the [typical set](https://en.wikipedia.org/wiki/Typical_set) or the space where the _probability mass_ (for our purposes here, this is synonymous with infinitesimal integrated chunks of _probability density_), becomes exponentially concentrated about a sphere about the origin. 

I showed the converse of this in my [post on hyperspherical and hyperbolic VAEs](/posts/2025/08/2025-11-27-constant-curvature-VAEs/), where if you sample uniformly on the unit sphere in high dimensions (~500) the distribution you get out in the marginals (the corner plot) is indistinguishable from those of a similarly dimensional gaussian.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-constant-curvature-vaes/HighDNormal/500D_multivariatenormal_corner.png" 
      style="width: 49%; height: auto; border-radius: 0px;">
  <img 
      src="/files/BlogPostData/2025-constant-curvature-vaes/HighDNormal/500D_uniform_on_sphere_corner.png" 
      style="width: 49%; height: auto; border-radius: 0px;" >

<figcaption>Comparison of a sample distribution on a uniform sphere and normal distribution in 500-dimensional space (not necessarily in that order)</figcaption>
</div>

So you get this kind of Faustian bargain where you can either:
 - Making the step size/width of the proposal ___small___, so that samples that are close/on the sphere stay close/on the sphere. But then exploring the hall space/distribution will be very inefficient, making the process take exponentially longer.
 - Making the step size/width of the proposal ___large___, so you can quickly explore the whole space, but have a low efficiency of having samples fall in the typical set, making the process take exponentially longer....

<br>


## Walking home drunk is not an efficient way to get home (kinda)


Closely linked to the above issue is how the efficiency of the stochastic process (sometimes referred to as a _drunken walk_ process) scales with dimension. For a $$d$$-dimensional distribution the efficiency of the process assuming an target efficiency of \~0.234 (23.4\%) (refs [Optimal scaling of discrete approximations to Langevin diffusions - Roberts, Rosenthal (1998)](https://academic.oup.com/jrsssb/article/60/1/255/7083121), [Investigating the Efficiency of the Metropolis Algorithm - Dou, Li, Wang (2024)](https://probability.ca/jeff/ftpdir/undergrads2024rep.pdf)) the optimal step size for RWM scales as $$d^{-1/2}$$ and efficiency (or number of steps) as $$\mathcal{O}(d)$$. 

Viewing this process as a diffusion process, where the efficiencies relate to how long it takes to reach an equilibrium distribution or a good enough approximation of one, then leaves the door open for us to wonder if other dynamics/processes could scale significantly better?

On possible algorithm that achieves this are the so-called "Langevin algorithms" that model the process via [Langevin dynamics](https://en.wikipedia.org/wiki/Langevin_dynamics). [Roberts & Tweedie (1996)](https://projecteuclid.org/journals/bernoulli/volume-2/issue-4/Exponential-convergence-of-Langevin-distributions-and-their-discrete-approximations/bj/1178291835.full) showed that the proposal distributions for Langevin-based algorithms could scale as $$d^{-1/3}$$ and the number of steps as $$\mathcal{O}(d^{1/3})$$! Which for high-dimensional sampling is a complete game changer when it comes to efficiency.

<br>


# Sneak Peek: An intuitive picture of Langevin Monte Carlo

Imagine the sampling trajectories of MCMC as people trying to find their way home after a _long_ night out. They are in their neighborhood, but they are delirious and have no sense of direction.

The RWM drunkards have lost their glasses in the mayhem of the night, and thus have completely failing eyesight. They pick a house at random nearby, stumble to the door, and are then close enough to check the house number on the door (the probability density). If that number is closer to their own address, they feel a sense of "positive feedback" and are likely to stay there for a moment. But then, forgetting everything, they stumble toward another random house. As this process repeats they have an unconcious positive feedback 'pull' towards the right house number. They are essentially "diffusing" through the neighborhood. Because they don't know which way the numbers are increasing or decreasing, they spend a massive amount of time walking in circles or heading toward the wrong end of the street.

The Langevin (LMC) Drunkard is just as tired, but theyve managed to keep their glasses on. They can actually _read_ the street signs. They notice the house numbers are increasing as they walk East, so they purposefully start walking East (the Drift). They are still drunk, so they still trip over the curb or weave side-to-side randomly (the Diffusion), but their average movement is a direct line toward their front door.

In a 1D street, the RWM drunkard might eventually find home. But imagine a 100-dimensional neighborhood (imagine Los Angeles in Blade Runner 2049 on steroids).

- The RWM drunkard is almost certain to take a step that leads them away from their house because there are so many "wrong" directions to choose from.

- The LMC drunkard, even with a slight stagger, has a compass (the Gradient). They ignore the 99 wrong directions and focus their energy on the one direction that actually matters.


More formally, LMC introduces a drift into the proposal distribution or gradient flow using the gradient of the target probability density but otherwise uses the same accept-reject as the typical MHMCMC algorithm. And even more formally ... is the rest of this post.

Below is a GIF showing how RWM and LMC converge on a target (same step size, set so that RWM is purposefully bad, but even then the LMC is fine).

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2026-01-LMC/rwm_path_density.gif" 
      style="width: 49%; height: auto; border-radius: 0px;"
      alt="RWM Walk"
      title="RWM Walk">
  <img 
      src="/files/BlogPostData/2026-01-LMC/lmc_path_density.gif" 
      style="width: 49%; height: auto; border-radius: 0px;" 
      alt="LMC Walk"
      title="LMC Walk"
      >
</div>

We will try to explain, motivate how we construct the below algorithm and that it in fact has an equilibrium distribution that matches our target.

>
#### Metropolis-Adjusted Langevin Algorithm
1. Initialise:
    - Have a target density you want to sample from $$f(x)$$ (known up to a normalising constant),
    - Define the potential energy as the negative log-density:
      $$U(x) = - \log f(x)$$,
    - Manually choose a starting point $$x_0$$,
    - Pick a step size $$\epsilon > 0$$,
    - Pick the number of samples to generate $$N$$.
2. For each iteration $$n$$ / Repeat $$N$$ times:
    - Langevin proposal steps:
         1. Compute the gradient of the log-density at the current state: $$\nabla \log f(x_n)$$
         2. Generate a proposal point $$x^*$$ using a drift term plus noise: $$x^* = x_n + \frac{\epsilon}{2} \nabla \log f(x_n) + \sqrt{\epsilon} \eta$$ where $$\eta \sim \mathcal{N}(0, I)$$
    - Metropolis Hastings correction steps:
         3. Compute the proposal densities:
           $$q(x^* \mid x_n) = \mathcal{N}\left(x^* ; x_n + \frac{\epsilon}{2} \nabla \log f(x_n),\epsilon I\right)$$
           $$q(x_n \mid x^*) = \mathcal{N}\left(x_n ; x^* + \frac{\epsilon}{2} \nabla \log f(x^*),\epsilon I\right)$$
         4. Compute the acceptance probability:
           $$\alpha = \min\left(1, \frac{f(x^*) q(x_n \mid x^*)}{f(x_n) q(x^* \mid x_n)}\right)$$
         5. Draw a random number $$u \sim \text{Uniform}(0, 1)$$
         6. Accept or reject:
            - If $$u \le \alpha$$, set $$x_{n+1} = x^*$$
            - If $$u > \alpha$$, set $$x_{n+1} = x_n$$

<br>


# Another Sneak Peek: An intuitive picture of Hamiltonian Monte Carlo


A much more commonly used algorithm (compared to LMC) is Hamiltonian Monte Carlo or _HMC_. Similar to LMC where we start thinking of the sampling procedure as some _stochastic_ process, HMC looks at the problem as a dynamic _deterministic_ process. They key idea being that you literally treat the problem as some ficticious _physical_ system where the samples of the parameters in the posterior $$\vec{\theta}$$ are akin to _position_ vectors, and we add some auxillary _momentum_ vectors $$\vec{r}$$.

We then define a Hamiltonian that defines the system, traversing equal energy (probability) contours, such that new proposed points will have similar or higher acceptance probabilities as the initial points, with a metropolis-hasting's-like adjustment so that the algorithm doesn't just turn into an optimisation routine. The Hamiiltonian is generally (like 99+\% of the time) as,

$$\begin{align}
\mathcal{H}(\vec{\theta}, \vec{r}) = U(\vec{\theta}) + K(\vec{r}),
\end{align}$$

where $$U$$ is our equivalent of potential energy, defined by $$U(\vec{\theta}) = - \log p(\vec{\theta}, \vec{x})$$, $$\vec{x}$$ being our data but is effectively treated as a constant here. The kinetic energy $$K$$ is then similarly defined as the negative of the log of the probability of a given momentum, typically chosen to just be a multivariate normal distribution with $$0$$ mean and unit variance. So, 

$$\begin{align}
K(\vec{r}) &= - \log p(\vec{r}) \\
&= - \log \mathcal{N}(\vec{0}, I) \\
&\propto - \log \exp\left(-\frac{1}{2} (\vec{r} - \vec{0})^T I^{-1} (\vec{r} - \vec{0}) \right) \\
&= \vec{r}^T \vec{r}/2.
\end{align}$$

This nicely matches up with actual (classical) physical systems where the kinetic energy is $$p^2/2m$$.

As in classical physics we can then evole the system according to[^dot],

[^dot]: I vehemently detest dot notation, despite most of the literature around this using it, I refuse.

$$\begin{align}
\frac{d}{dt}\vec{\theta} = \frac{\partial H(\vec{\theta}, \vec{r})}{\partial \vec{r}} \\
\frac{d}{dt}\vec{r} = - \frac{\partial H(\vec{\theta}, \vec{r})}{\partial \vec{\theta}}.
\end{align}$$

If unfamiliar, part of the reason it is we model physical systems this way is because this then enforces that energy is conserved/the derivative of the total energy with respect to time is 0,

$$\begin{align}
\frac{d}{dt} H(\vec{\theta}, \vec{r}) &= \frac{\partial H(\vec{\theta}, \vec{r})}{\partial \vec{\theta}} \frac{d}{dt}\vec{\theta} + \frac{\partial H(\vec{\theta}, \vec{r})}{\partial \vec{r}}\frac{d}{dt}\vec{r}\\
&= \frac{\partial H(\vec{\theta}, \vec{r})}{\partial \vec{\theta}} \frac{\partial H(\vec{\theta}, \vec{r})}{\partial \vec{r}} - \frac{\partial H(\vec{\theta}, \vec{r})}{\partial \vec{r}}\frac{\partial H(\vec{\theta}, \vec{r})}{\partial \vec{\theta}} \\
&= 0.
\end{align}$$

This leads us to essentially modelling a physical system, where previously we were modelling a markov process on a probability density. The potential of the system is the negative log probability of the distribution we want, and the proposal samples are generated by traversing along equal energy contours/the paths particles would take in a physical system. There's just a few things regard how long the paths are between proposals, how big the steps are in these traversals, monte carlo adjustments related to making a discrete approximation to a continuous system and reversing the momentum after accepting a sample. 

A naive implementation is shown in the GIF below.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2026-01-LMC/hmc_visualization.gif" 
      style="width: 100%; height: auto; border-radius: 0px;">
</div>

(Although now I realise the labels for the contours of the KDE estimate are the wrong way around...)
<br>
<br>

# Some mathematical background

In this section we'll go through the required math particularly to understand Langevin Dynamics and the Fokker-Planck Equation. If you either: are not fussed about the nitty-gritty details or are already familiar with the topics above you can skip this section. I endeavoured to make the other sections not _rely_ on this section but I will refer back to them every now and then for details. 

<br>


## Stochastic Differential Equations (SDEs) 


You can essentially describe SDEs as an extension to ODEs that includes some noise or stochastic component (for the purpose of this blog post at least). 

In essence, where you would have an ODE described by:

$$\begin{align}
\begin{cases}
\frac{dx}{dt}(t) = b(x(t)) & (t\geq 0) \\
x(0) = x_0
\end{cases}
\end{align}$$

that described some smooth trajectory $$x(t)$$ ($$x$$ some stand-in for some arbitrary quantity) or progression of states for times $$\geq 0$$, that may look like the following for example,

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2026-01-LMC/exp_ode_example.png" 
      style="width: 49%; height: auto; border-radius: 0px;"
      alt="ODE Example"
      title="ODE Example">
</div>


If you add a noise component, that may look like,

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2026-01-LMC/exp_sde_example.png" 
      style="width: 49%; height: auto; border-radius: 0px;"
      alt="SDE Example"
      title="SDE Example">
</div>

such that the system can only now be adequately described by,

$$\begin{align}
\begin{cases}
\frac{dy}{dt}(t) = b(y(t)) + B(y(t)) \xi(t) & (t\geq 0) \\
y(0) = y_0
\end{cases}
\end{align}$$

where $$B:\mathbb{R}^n \rightarrow \mathbb{M}^{n \times m}$$ (space of $$n\times m$$ matrices) and $$\xi(t)$$ an $$m$$-dimensional vector function that would be described as 'white noise' (with now $$y$$ now some stand-in for some arbitrary quantity to highlight the difference with $$x$$ the smooth ODE result).

The white noise term is often replaced with the time derivative of a ["Wiener process"/brownian motion process](https://en.wikipedia.org/wiki/Wiener_process)[^wiener] $$\frac{dW}{dt} = \xi(t)$$ which then allows us to express the above in differential form as below.

[^wiener]: Wiener named after Norbert Wiener is pronounced with a "v" but that doesn't stop me 100% reading as a "w" to make the papers I read funnier, and yes, I am a child.

$$\begin{align}
\begin{cases}
dy = b(y(t)) dt + B(y(t)) dW(t) & (t\geq 0) \\
y(0) = y_0
\end{cases}
\end{align}$$

Thus the general solution of the SDE is then "just",

$$\begin{align}
y(t) = y_0 + \int_{s=0}^{s=t} b(y(s)) ds +  \int_{s=0}^{s=t} B(y(s)) dW(s).
\end{align}$$

But for anyone vaguely familiar with Brownian motion it is everywhere continuous and nowhere differentiable so $$\frac{dW}{dt}$$ for sure doesn't really exist. So:

1. What even is $$dW$$?
2. What does $$\int_{s=0}^{s=t} dW(s)$$ mean?
3. Is the equation for $$y(t)$$ actually solvable?

And so we begin.

<br>


### What even is $$dW$$?

Skipping some motivation and background we can define the process $$W(t)$$ as:

>
___Wiener Process Definition___.
1. The process $$W$$ has the initial condition $$W(0) = 0$$,
2. $$W(t) - W(s) ~ \mathcal{N}(0, t-s), \forall \; t\geq s \geq 0$$,
3. $$\forall t$$ where $$0 \lt t_1 \lt t_2 \dots \lt t_n$$, the increments/differences $$W(t_{j+1}) - W(t_j)$$ are independent


So we have $$W(t)$$, or at least some abstract definition for it. This is basically just describing any process where some particle/object/thing jiggles, such that it's particular position at any point in time can be represented probabilistically as some gaussian with variance given as the difference in time between either the starting time or some reference point.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2026-01-LMC/BrownianMotionGIF.gif" 
      style="width: 89%; height: auto; border-radius: 0px;"
      alt="Brownian Motion 'Jiggle' Example"
      title="Brownian Motion 'Jiggle' Example">
</div>

In standard calculus, $$dx$$ is just the infinitesimal change in $$x$$. In our stochastic process world now, $$dW$$ is closer to an infinitesimal 'shock'[^shock] rather than some strict notion of 'change' if that makes sense (which complicates how we're meant to do the integrals above).

[^shock]: I'm using a lot of '\_' and "\_" in this blog post because I am skipping the rigor to just say things...

But similar to the "smooth" calculus $$dx$$, we take the limit $$\Delta t \to dt$$ with $$\Delta W = W(t + dt) - W(t)$$, to get some of the properties of the "stochastic differential" $$dW$$:
 - Expectation is zero: $$E[dW] = 0$$. On average, the noise doesn't "push" the particle in any particular direction, on average it stays where it started.
 - The "Square" Rule: In standard calculus, $$(dt)^2$$ is so small we often ignore it. But for a Wiener process: $$(dW)^2 \approx dt$$ (more strictly $$E[(dW)^2] = dt$$ but :shrug:).

<br>

## Stochastic Integrals: It么 Calculus vs. Stratonovich

If we try to solve the integral term $$\int B(y(s)) dW(s)$$ using standard Riemann sums (the "rectangles under the curve" method you learned in high school), we break math. 

Why? 

Because the path of $$W(t)$$ is so jaggedit has infinite variationthe answer depends on where in the rectangle you measure the height.

We have two main choices, which essentially boil down to: "When does the noise hit?"


1.___The Ito Integral (Common Financial/CS Choice)___ 

In the Ito (or the correctly spelled It么) interpretation, we evaluate the function at the beginning of the time step, kind of like if you were trying a model a situation in the wild, you only have the information up to the current timestep, you don't know the information in half a timestep's time, let alone a full one. 

$$\int_0^t B(s) dW(s) \approx \sum_{i} B(t_i) (W(t_{i+1}) - W(t_i))$$

This is the standard for finance and computer science. It has the "_martingale_" property: the noise at the current step is independent of the state, meaning you can't use the noise to "peek" into the future ([Sean Dineen's](https://www.amazon.com.au/Probability-Theory-Finance-Mathematical-Black-Scholes/dp/0821839519) textbook on probability and SDEs has a more nice in-depth explanation for this). However, it breaks the chain rule of standard calculus. 

To differentiate functions of stochastic processes, you need ___Ito's Lemma___ (the Taylor Series for SDEs):

$$df = \left( f'(y)b(y) + \frac{1}{2}f''(y)B(y)^2 \right)dt + f'(y)B(y)dW$$

Remembering that $$(dW)^2 \approx dt$$, we get _extra terms_ that wouldn't exist in normal calculus.

2.___The Stratonovich Integral (The "Physics" Choice)___ 

Here, we evaluate the function at the midpoint of the time step: 

$$\frac{t_i + t_{i+1}}{2}.$$

Giving the definition of the integral to be:

$$
\int_0^t B(s) dW(s) \approx \sum_{i} B\left( \frac{t_i+ t_{i+1}}{2}\right) (W(t_{i+1}) - W(t_i)).
$$

This formulation preserves standard calculus rules (the chain rule works as expected!), making it popular in physics. 

However, it secretly looks into the future (the midpoint uses information from $$t_{i+1}$$), making it computationally awkward for simulations where causality matters.

Again, kind of by construction for a discrete process like making investments in a finance, you are stuck in the present, you _shouldn't_ be using information of the future.

<div style="text-align: center;">
  <img 
      src="https://media1.tenor.com/m/-CV1ItBhotMAAAAC/we-dont-really-know-what-the-future-holds-asher-angel.gif" 
      style="width: 69%; height: auto; border-radius: 0px;">
</div>

<br>


TL;DR: We will stick to Ito for simplicity in sampling algorithms, but we have to be careful with our calculus rules. Below is a gif showing some an example difference between the two for the same path.
<br>

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2026-01-LMC/ito_vs_stratonovich.gif" 
      style="width: 99%; height: auto; border-radius: 0px;">
</div>

And then just for the last frame so you don't have to watch the gif over and over again.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2026-01-LMC/ito_vs_stratonovich_final.png" 
      style="width: 99%; height: auto; border-radius: 0px;">
</div>


## The Langevin SDE: Combining Deterministic Drift and Random Fluctuations

Now we can define the specific SDE that this part of this post is meant to actually be about.

Langevin Dynamics models the movement of a particle in a fluid. It is subject to two forces:
- Drag/Friction: It wants to stop moving or settle into a low-energy state.
- Thermal Fluctuation: The random collisions with fluid molecules (the "noise") kicking it around.

In the context of sampling (e.g., finding the parameters of a Bayesian model), we treat the "energy landscape" as the negative log-probability of our target distribution (as I defined above for the sneak peak in HMC). 

We want the particle to settle in the high-probability regions (low energy) but keep jiggling to explore. We use the _Overdamped Langevin Equation_, where we assume the friction is so high we can ignore mass and acceleration (inertia)[^underdamped]. 

[^underdamped]: As opposed to the actually more commonly used "under"-damped Langevin sampler which doesn't ignore mass and models momenta

The velocity is proportional to 

$$dY_t = -\nabla U(Y_t) dt + \sqrt{2D} dW_t,$$ 

which is dependent on:
- the state/position $$Y_t$$
- the Drift: $$-\nabla U(Y_t)$$. This pushes the particle downhill toward the minimum of the potential energy $$U(x)$$.
- and finally the diffusion $$\sqrt{2D} dW_t$$: This is the thermal noise that prevents the particle from just getting stuck in the nearest local minimum. i.e. $$D$$ controls the "temperature" and is assumed to be a constant diagonal matrix (with the square root an element-wise operation or D is just a constant $$\in \mathbb{R}$$)

A gif showing the combination of the effects is below.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2026-01-LMC/langevin_dynamics.gif" 
      style="width: 99%; height: auto; border-radius: 0px;">
</div>


## The Fokker-Planck Equation: "Flowing" Probability?

So far, we've looked at the path of a single particle. But since the movement is random, what if we ran this simulation a million times? Or what if we had a million particles? Or more simply, how do we know if a _distribution_ is the steady-state or final solution of our dynamics?

But since the movement is random, looking at one squiggly line doesn't tell us the whole story.

Instead of tracking the position of one particle, let's track the probability density $$p(x, t)$$. This tells us the probability of finding the particle at position $$x$$ at time $$t$$. 

If we want to see how this probability cloud evolves over time, we need a Partial Differential Equation (PDE). 

For this we can imagine that the probability is like a fluid: it cannot be created or destroyed, it can only flow from one place to another. 

This is mathematically described by the Continuity Equation:

$$\frac{\partial p(x, t)}{\partial t} = -\nabla \cdot J(x, t)$$

Where $$\frac{\partial p}{\partial t}$$ is the change in probability at a specific spot, $$J$$ is the Probability ['Flux'](https://en.wikipedia.org/wiki/Flux) (or 'current') describing where the probability is flowing, $$\nabla \cdot J$$ is the the 'divergence', measuring how much "stuff" is leaving a point (but can also be negative, so also measures stuff entering a point). 

More simply the equation says: "The amount of probability at a point changes only if probability flows in or out of that point." Nothing will magically appear or dissapear and the sum of derivatives of the [flux](https://en.wikipedia.org/wiki/Flux) of stuff with respect to space is directly proportional to the time derivative of the total amount of stuff moving in and out the neighbourhood of a given point.

I tried to "GIF"-ify this, but I'm not sure how effective it is. There are two plots below showing a ball of 'probability' moving through a box/space (you can think of this as the neighbourhood of a given point) and then an abstract fluid.

<br>
___ball of stuff___


<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2026-01-LMC/continuity_ball.gif" 
      style="width: 99%; height: auto; border-radius: 0px;">
</div>

<br>
<br>

___abstract fluid___

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2026-01-LMC/continuity_equation_abstract.gif" 
      style="width: 99%; height: auto; border-radius: 0px;">
</div>


<br>

Although while trying to find some inspiration for GIFs to make that would be better than the above or require me to learn manim, I found this meme where the bit this guy is writing is this exact concept . Which I think is as good as I can do (for those wondering, no it is _not_ simple math...).


<div style="text-align: center;">
  <img 
      src="https://media1.tenor.com/m/uhQs_zJwL9AAAAAd/doug-maclean-nhl.gif" 
      style="width: 59%; height: auto; border-radius: 0px;">
</div>

<br>

For a better resource on the integral formulation and differentual formulation of the continuity equation (we are using the differential form) I'd recommend [this video by Steve Brunton](https://www.youtube.com/watch?v=Fk3X6o39KEI&pp=ygUgZGlmZmVyZW50aWFsIGNvbnRpbnVpdHkgZXF1YXRpb24%3D).


In our Stochastic Differential Equation, the flux $$J$$ is made of two competing parts:
- The 'Advective' Flux (Drift): The deterministic force $$b(x)$$ pushing the particle.$$J_{\text{drift}} = b(x) p(x, t)$$. This is the "River" part. If you just had this (no noise), you would recover the general form of Liouville's Equation for deterministic flow.
- The 'Diffusive' Flux: The random noise spreading things out. By [Fick's Law](https://en.wikipedia.org/wiki/Fick%27s_laws_of_diffusion), stuff naturally flows from high concentration to low concentration. $$J_{\text{diffusion}} = - \frac{D(x, t)}{2} \nabla p(x, t)$$. This is the "Dye Spreading" part.

If we just plug our total flux $$J = J_{\text{drift}} + J_{\text{diffusion}}$$ back into the continuity equation, we get the Fokker-Planck Equation:

$$\frac{\partial p}{\partial t} = -\nabla \cdot \left[ \underbrace{b(x) p}_{\text{Drift}} - \underbrace{\frac{D(x, t)}{2} \nabla p}_{\text{Diffusion}} \right]$$

Expanding the divergence gives us the standard form:

$$\frac{\partial p}{\partial t} = \underbrace{-\nabla \cdot (b(x) p)}_{\text{Drift pushes } p} + \underbrace{\frac{1}{2} \nabla^2 [D(x, t) p]}_{\text{Diffusion spreads } p}$$

For our Dynamics we'll say $$b(x) = -\nabla U(x)$$: 

$$\frac{\partial p}{\partial t} = \nabla \cdot (\nabla U(x) p) + \frac{1}{2} \nabla^2 [D(x, t) p]$$ 

Conceptually: The Drift tries to compress the probability mass into the minimum of the energy $$U(x)$$, while the Diffusion tries to flatten the probability mass out (entropy). 

When these two fluxes balance out ($$J_{total} = 0$$), we hit our stationary distribution.

## The MCMC Checklist

Now, if we discretize our SDEs to run it on a computer (which we call the [Euler-Maruyama method](https://en.wikipedia.org/wiki/Euler%E2%80%93Maruyama_method)), we are essentially creating a Markov Chain. 

For this chain to be useful for sampling, it needs to satisfy three core properties.

__1.Ergodicity (The "No Island" Rule)__

Ergodicity is the guarantee that the particle can eventually reach any point in the space, no matter where it starts.

- In math terms: The chain must be irreducible and aperiodic.

- In conceptual terms: If your energy landscape $$U(x)$$ has two deep valleys separated by an infinitely high wall, your algorithm isn't ergodic. It will get stuck in one valley and never "see" the other, giving you a biased view of the distribution. The noise term $$dW$$ is what provides ergodicity by allowing the particle to climb over (or tunnel through) energy barriers.

__2.Detailed Balance (Reversibility)__

This is the most "physical" requirement. It is also actually more robust than what is strictly needed (which is global balance) but this is typically what people try to prove their algorithm satisfies. 

A Markov chain is said to satisfy detailed balance if, at equilibrium, the probability of being at state $$A$$ and moving to $$B$$ is exactly equal to the probability of being at $$B$$ and moving to $$A$$.

$$p(x) T(x \to x') = p(x') T(x' \to x)$$

where $$T$$ is the transition probability. In the context of the Fokker-Planck equation, this is equivalent to saying the total flux $$J$$ is zero everywhere, not just that its divergence is zero. 

If detailed balance holds, there are no "circular currents" in your probability fluid. Its a "static" equilibrium rather than a "dynamic" one. And essentially this also means that once the "probability fluid" reaches the shape (probability distribution), it stays the correct shape shape.


### The Discretization Gap

This is all good but while the continuous description of our SDE can satisfy these properties, the discrete version we run on a computer (taking finite steps $$\Delta t$$) actually introduces a slight error.

Because we aren't moving in infinitesimally small steps, we slightly "overshoot" the curves of the energy landscape (even if we aren't explicitly traversing it with gradients we're still moving along it). 

This means that the stationary distribution of our computer code is actually slightly different from our target $$\pi(x)$$.

To fix this, we often add a ___Metropolis-Hastings "accept/reject" step___ at the end of each jump to restore detailed balance. 

e.g. This turns "Unadjusted Langevin Algorithm" (ULA) into the "Metropolis-Adjusted Langevin Algorithm" (MALA) which is the one used in practice. 

___In summary___

| Property | Physical Intuition | Why we need it |
| :--- | :--- | :--- |
| Stationarity | The fluid has settled into its final shape. | Ensures we are sampling the right distribution. |
| Ergodicity | The particle can visit every "room" in the house. | Ensures we don't miss any peaks in the data. |
| Detailed Balance | No hidden whirlpools; every move is reversible. | Simplest way to guarantee stationarity. |

Another point that one would like to know is the ___mixing time___ or convergence rate which relates to _how long_ it will take for a sampler to converge. We'll just leave this at intuitive explanations rather than try to prove convergence rates. I don't quite have the time to explain absolutely everything in this post (and in fact I'm trying to condense multiple textbooks worth of material into what I have above so any more and I might just explode or simply regurgitate all of said textbooks).


# Hamiltonian MCMC and NUTS in detail

For this section I will first introduce Hamiltonian Monte Carlo (HMC) and then NUTS which is kind of 'auto-tuning' method that makes HMC more user friendly. I will be very heavily using parts of [_"MCMC using Hamiltonian dynamics"_ - Radford M. Neal](https://arxiv.org/abs/1206.1901) for the HMC section and [_"The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo"_ - Hoffman and Gelman](https://arxiv.org/abs/1111.4246) for the NUTS section (both papers are those that introduced the respective methods). Additionally I'd recommend ["A Conceptual Introduction to Hamiltonian Monte Carlo" - Michael Betancourt](https://arxiv.org/abs/1701.02434) if you wanted some additinoal info.

## The original HMC algorithm

As a quick refresher, in HMC we define a Hamiltonian as,

$$\begin{align}
H(\vec{\theta}, \vec{r}) = U(\vec{\theta}) + K(\vec{r}) = -\log\left(p(\vec{\theta}) \right) + \frac{r^T M^{-1} r}{2},
\end{align}$$

where compared to the section above I'm using $$M$$ which is positive-definite matrix called the "mass matrix" which as in the name, takes a similar role as mass in standard physical systems. Within the probabilistic interpretation of $$K(\vec{r})$$ the mass matrix takes the role of the momentum distribution's (normal distribution) covariance matrix. The system's [Hamiltonian dynamics](https://en.wikipedia.org/wiki/Hamiltonian_mechanics) are then dictated by the following Hamiltonian equations,

$$\begin{align}
\frac{d\vec{\theta}}{dt} = M^{-1} \vec{r} \\
\frac{d\vec{r}}{dt} = \frac{\log p(\vec{\theta})}{\partial \vec{\theta}}. \\
\end{align}$$

We then want to use this as a way to traverse the probability density to propose distant points that have similar acceptance probabilities. This is what increases the efficiency of the HMC algorithm compared to Metropolis-Hasting MCMC as we can make more informed proposals. 

We then want to make it so that the proposals are far away from the previous so that we can efficiently explore the space and not over sample some trajectories. Together this motivates (but doesn't fully explain quite yet) the below algorithm for a diagonal mass matrix;


>
#### _Hamiltonian Monte Carlo_ Algorithm
1. Initialise:
- Have a target density you want to sample from $$p(\vec{\theta})$$ (known up to a normalising constant)
- Define the potential energy: $$U(\vec{\theta}) = -\log p(\vec{\theta})$$
- Define the kinetic energy (usually Gaussian): $$K(\vec{r}) = \frac{1}{2} \vec{r}^T M^{-1} \vec{r}$$, where $$M$$ is a mass matrix (often $$I$$ or $$\textrm{diag}[m_1, m_2, ..., m_d]$$)
- Manually choose a starting point $$\vec{\theta}_0$$
- Pick a step size $$\epsilon \in \mathbb{R}^+$$ and the number of leapfrog steps $$L\in\mathbb{N}$$.
- Pick the number of samples to generate $$N$$.
2. Sample:
For each iteration $$n$$ / Repeat $$N$$ times:
    1. Resample momentum: Draw a fresh momentum vector $$\vec{r}_n \sim \mathcal{N}(0, M)$$.
    2. Leapfrog Integration (Simulate Dynamics):
        - Set initial state for the trajectory: $$(\vec{\theta}^*, \vec{r}^*) = (\vec{\theta}_n, \vec{r}_n)$$.
        - Perform the first half-step for momentum: $$\vec{r}^* \leftarrow \vec{r}^* - \frac{\epsilon}{2} \nabla U(\vec{\theta}^*)$$
        - For $$l$$ from $$1$$ to $$L$$:
            - Update position: $$\vec{\theta}^* \leftarrow \vec{\theta}^* + \epsilon M^{-1} \vec{r}^*$$
            - If $$l < L$$, update momentum: $$\vec{r}^* \leftarrow \vec{r}^* - \epsilon \nabla U(\vec{\theta}^*)$$
        - Perform the final half-step for momentum: $$\vec{r}^* \leftarrow \vec{r}^* - \frac{\epsilon}{2} \nabla U(x^*)$$
    3. Metropolis-Hastings Correction:
        - Compute the Hamiltonian (total energy) at the start and end: $$H(\vec{\theta}_n, \vec{r}_n) = U(\vec{\theta}_n) + K(p_n)$$$$H(\vec{\theta}^*, \vec{r}^*) = U(\vec{\theta}^*) + K(\vec{r}^*)$$
        - Compute the acceptance probability: $$\alpha = \min\left(1, \exp(H(\vec{\theta}_n, \vec{r}_n) - H(\vec{\theta}^*, \vec{r}^*))\right)$$
        - Draw a random number $$u \sim \text{Uniform}(0, 1)$$.
        - Accept or reject: If $$u \le \alpha$$, set $$\vec{\theta}_{n+1} = \vec{\theta}^*$$. If $$u > \alpha$$, set $$\vec{\theta}_{n+1} = \vec{\theta}_n$$

<br>

The trajectories share elements of (/are) [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling). Which if you're unfamiliar, is an MCMC method where if you have a distribution such as $$p(\theta_1, \theta_2, ..., \theta_d)$$ then you can sample it by iteratively sampling the conditionals $$p(\theta_i\vert\theta_1, \theta_2, ..., \theta_{i-1}, \theta_{i+1}, ..., \theta_d) = p(\theta_i \vert \theta_{-i})$$ where we define $$\theta_{-i}$$ as all $$\theta_j$$ where $$i\neq j$$. 

I'll save a detailed explanation of Gibbs sampling for another time[^Gibbsref]. I've included a GIF below of the method in a few test cases to give an idea of _how_ it works and I'm thinking in the future I'll go into a bit more depth with Gibbs sampling and a kind of evolution of Gibbs sampling called [Slice sampling](https://en.wikipedia.org/wiki/Slice_sampling) in another post (or maybe I just wanted to make the GIF).

[^Gibbsref]: If you're looking for a ref a standard one is ["Explaining the Gibbs Sampler" by George Casella and Edward George](http://markirwin.net/stat220/Refs/casellageorge1992.pdf) (did not realise they both had George in their names until now) but to me felt a bit lacking in rigor and generality. Honestly the ["Implementation" to "Mathematical background" (inclusive) sections of the Gibbs sampling page](https://en.wikipedia.org/wiki/Gibbs_sampling#:~:text=of%20conditional%20distributions.-,Implementation,-%5Bedit%5D) has a bit more generality and is quite good.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2026-01-LMC/gibbs_sampler.gif" 
      style="width: 99%; height: auto; border-radius: 0px;">
</div>


<br>

Now that we have our algorithm, let's check that it is a valid MCMC scheme by checking whether it satisfies _detailed balance_ and is _ergodic_.

### Detailed balance

#### Method 1

To satisfy detailed balance, the Hamiltonian transition must be ___reversible___ and ___volume-preserving___.

We define a transition as $$f(\mathbf{\theta}, \mathbf{r}) = (\mathbf{\theta}^*, \mathbf{r}^*)$$ using the Leapfrog Integrator. 

##### Reversibility: 

For the MH step to work, the proposal must be reversible. If we propose a move from $$A \to B$$, we must be able to move $$B \to A$$ with the same mechanism.

The Leapfrog integrator is mathematically reversible: if you negate the momentum at the end of a trajectory ($$\mathbf{r}^* \to -\mathbf{r}^*$$) and run the integrator again, you return exactly to the starting position $$\mathbf{\theta}$$.

##### Volume Preservation (Symplectic Property)

In MH, the acceptance ratio usually includes a Jacobian term $$\vert J \vert$$ to account for the change in coordinate volume.

However, the Hamiltonian flow (and the Leapfrog integrator) is symplectic, meaning the Jacobian of the transformation is exactly 1. 

$$\text{det}\left( \frac{\partial(\mathbf{\theta}^*, \mathbf{r}^*)}{\partial(\mathbf{\theta}, \mathbf{r})} \right) = 1$$

This simplifies the MH acceptance ratio significantly, as the volume doesn't "stretch" or "compress."

##### The MH Acceptance Step

Combining these, we propose a state $$(\mathbf{\theta}^*, \mathbf{r}^*)$$ and accept it with probability:

$$\alpha = \min\left(1, \frac{\pi(\mathbf{\theta}^*, \mathbf{r}^*)}{\pi(\mathbf{\theta}, \mathbf{r})}\right) = \min\left(1, \exp(-H(\mathbf{\theta}^*, \mathbf{r}^*) + H(\mathbf{\theta}, \mathbf{r}))\right)$$

Because the integrator is reversible and volume-preserving, this MH step ensures that the transition satisfies detailed balance with respect to the joint distribution $$\pi(\mathbf{\theta}, \mathbf{r})$$.

#### Method 2 / Going into a tinsy more depth

If you treat the whole system as a joint probability over $$\vec{\theta}$$ and $$\vec{r}$$ with the negative log probability given as the hamiltonian $$H(\vec{\theta}, \vec{r})$$ thus $$p(\vec{\theta}, \vec{r}) = \exp(-H(\vec{\theta}, \vec{r}))$$ then we can examine the transition probability through the MH acceptance step seeing as the traversal is deterministic, reversible and volume preserving (by the usage of the Leapfrog integrator). Directly examining the detailed balance with transitioning from $$(\vec{\theta}, \vec{r})$$ to $$(\vec{\theta}^*, \vec{r}^*)$$ or $$z$$ to $$z^*$$ (so I don't have to write as much) we find:

$$\begin{align}
p(\vec{\theta}, \vec{r}) \cdot p(\vec{\theta}^*, \vec{r}^* \vert \vec{\theta}, \vec{r}) &= C e^{-H(z)} q(z\rightarrow z^*) \min\left(1, \exp(-H(z^*) + H(z))\right) \\
\end{align}$$

Examining each case

##### Case 1: $$\min\left(1, \exp(-H(z^*) + H(z))\right) = 1$$

$$\begin{align}
p(\vec{\theta}, \vec{r}) \cdot p(\vec{\theta}^*, \vec{r}^* \vert \vec{\theta}, \vec{r}) &= C e^{-H(z)} q(z\rightarrow z^*) \\
&= C e^{-H(z)} q(z^* \rightarrow z) \\
&= C e^{-H(z^*)} q(z^* \rightarrow z) \cdot \min\left(1, \exp(-H(z) + H(z^*))\right) \\
&= p(\vec{\theta}^*, \vec{r}^*) \cdot p(\vec{\theta}, \vec{r} \vert \vec{\theta}^*, \vec{r}^*)
\end{align}$$

Where by virtue of the reversibility of the traversal $$q(z^* \rightarrow z) = q(z\rightarrow z^*)$$ and if $$\min\left(1, \exp(-H(z^*) + H(z))\right) = 1$$ then $$H(z^*) < H(z)$$ therefore $$\min\left(1, \exp(-H(z) + H(z^*))\right) = \exp(-H(z) + H(z^*))$$.


##### Case 2: $$\min\left(1, \exp(-H(z^*) + H(z))\right) = \exp(-H(z^*) + H(z))$$

Similarly to above, 

$$\begin{align}
p(\vec{\theta}, \vec{r}) \cdot p(\vec{\theta}^*, \vec{r}^* \vert \vec{\theta}, \vec{r}) &= C e^{-H(z)} q(z\rightarrow z^*) \exp(-H(z^*) + H(z))\\
&= C e^{-H(z^*)} q(z^* \rightarrow z) \\
&= C e^{-H(z^*)} q(z^* \rightarrow z) \cdot \min\left(1, \exp(-H(z) + H(z^*))\right) \\
&= p(\vec{\theta}^*, \vec{r}^*) \cdot p(\vec{\theta}, \vec{r} \vert \vec{\theta}^*, \vec{r}^*)
\end{align}$$

Where if $$\min\left(1, \exp(-H(z^*) + H(z))\right) = \exp(-H(z^*) + H(z))$$ then $$H(z^*) > H(z)$$ therefore $$\min\left(1, \exp(-H(z) + H(z^*))\right) = 1$$.

Hence, we have detailed balance.


### Ergodicity

Ergodicity requires the chain to be irreducible (can reach any state) and aperiodic.

#### Irreducibility: 

Since we refresh the momentum $$\mathbf{r}$$ from a Gaussian distribution at every step, we can potentially gain enough "energy" to reach any part of the state space, provided the potential $$U(\theta)$$ is continuous and the gradient is nicely behaved.

#### Aperiodicity: 

Because we accept/reject based on a Metropolis step and the momentum is drawn from a continuous distribution, the probability of the chain being trapped in a cycle is generally zero. 

However, a common pitfall in plain HMC (the "original" HMC algorithm described above with fixed $$L$$) is the "resonant period." If the trajectory length $$L$$ is exactly synchronized with the period of an orbit in the distribution, the sampler might stay on a specific contour. Modern implementations like NUTS (No-U-Turn Sampler) solve this by dynamically choosing $$L$$ to avoid such cycles, ensuring robust ergodicity.


So, HMC is a valid MCMC method. And an example implementation (same as in the intro can be seen in the GIF below).

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2026-01-LMC/hmc_visualization.gif" 
      style="width: 100%; height: auto; border-radius: 0px;">
</div>
<br>

The things to note being:
- HMC does not accept every single point along a travejectory
- HMC is not traversing equal probability contours, just equal energy which is a combination of the parameter probability contours and the momentum probability contours
- HMC is very efficient in that it's accepting almost every proposal

## Now let's go NUTS: Adaptively tuning for step length

Now HMC seems great, and it is, but there's one huge issue with it: the sensitivity to the traversal length $$L$$ hyperparameter. As stated above, if you pick a bad $$L$$ you can either: traverse the density slowly ($$L$$ too small) or become periodic/too big ($$L$$ too big). There's a relatively narrow window of 'good' choices for $$L$$ that typically require one to run some preliminary runs with different $$L$$ values that can be quite expensive especially if you're likelihood is very localised and you aren't quite sure where the majority probability mass resides. 

What would be great is if we could have the $$L$$ adaptively set without user input. And that's what the No-U-Turn Sampler (NUTS) provides. 

The main idea is that once the sampler starts _turning_ back on itself (i.e. the traversal's time derivative with respect to the starting point is negative) then you are wasting samples as more samples means that they are closer to the starting point. And we want to maximise this distance as it means we explore more of the distribution i.e. increase the mixing rate/convergence rate. 

The question is then how do we mathematically encode this idea? 


### Binary tree construction and leaves coming together


The method described in the original paper was to notice that for unit mass the time derivative of the magnitude of the given iteration of the trajectory $$\tilde{\mathbf{\theta}}$$ and the last proposal $$\mathbf{\theta}$$:

$$\begin{align}
\frac{d}{dt} \frac{(\tilde{\mathbf{\theta}} - \mathbf{\theta}) \cdot (\tilde{\mathbf{\theta}} - \mathbf{\theta})}{2} = (\tilde{\mathbf{\theta}} - \mathbf{\theta}) \cdot \frac{d}{dt} (\tilde{\mathbf{\theta}} - \mathbf{\theta}) = (\tilde{\mathbf{\theta}} - \mathbf{\theta}) \cdot \tilde{\mathbf{r}}.
\end{align}$$

So, in their original paper for HMC-NUTS [Hoffman and Gelman](https://arxiv.org/abs/1111.4246) suggest that we can then say that when scalar product of the given value of the momentum and the difference above is less than 0 that the trajectories are circling back and wasting evaluations but that just implementing it by running the leapfrog steps until the condition is satisfied wouldn't be reversible. This is fixed by using a doubling procedure shown in the figure directly below. 


<div style="text-align: center;">
  <h3 style="margin-bottom: 10px;">Recursive Doubling and Binary Tree Search in MCMC</h3>
  <img 
      src="/files/BlogPostData/2026-01-LMC/HoffmanGelman_Binary_Tree.png" 
      alt="Binary Tree Visualization from Hoffman and Gelman's paper introducing HMC"
      style="width: 100%; height: auto; border-radius: 0px;">
  <p style="font-size: 0.9em; color: #555; margin-top: 10px; font-style: italic;">
    <strong> Figure: The binary tree structure representing the recursive doubling procedure used in HMC-NUTS from the paper "The No-U-Turn Sampler: Adaptively Setting Path Lengths  in Hamiltonian Monte Carlo" by Hoffman and Gelman (2011). The caption reads</strong> -"Example of building a binary tree via repeated doubling. Each doubling proceeds by choosing a direction (forwards or backwards in time) uniformly at random, then simulating Hamiltonian dynamics for 2j leapfrog steps in that direction, where j is the number of previous doublings (and the height of the binary tree). The figures at top show a trajectory in two dimensions (with corresponding binary tree in dashed lines) as it evolves over four doublings, and the figures below show the evolution of the binary tree. In this example, the directions chosen were forward (light orange node), backward (yellow nodes), backward (blue nodes), and forward (green nodes)."
  </p>
</div>
<br>

And maybe in slightly different terms: We expand out from our initial proposal in opposite directions (via Hamiltonian dynamical leaps), randomly picking which one for any given iteration of a doubling procedure we we sucessively take more and more steps in a given doubling until our algorithm tells us to stop. We then find that the adjusted algorithm after some checks[^checks] is shown below:

[^checks]: that I won't show here as I'm not familiar enough with them to do much beyond [Hoffman and Gelman](https://arxiv.org/abs/1111.4246) did anyways, so I'd once again suggest heading on over there for the details

>
#### ___Naive No-U-Turn Sampler (NUTS) Algorithm___
- Initialise:
    - Have a target log-density you want to sample from $$\mathcal{L}(\theta) = \log p(\theta)$$.
    - Manually choose a starting point $$\theta_0$$.
    - Pick a step size $$\epsilon \in \mathbb{R}^+$$.
    - Pick the number of samples to generate $$K$$.
    - Define a threshold $$\Delta_{max}$$ ('typically' $$1000$$) to identify Divergences.
- Sample:
For each iteration $$k$$ from $$1$$ to $$K$$:
    - Resample momentum: Draw $$r_0 \sim \mathcal{N}(0, I)$$.
    - Slice sampling: Draw $$u \sim \text{Uniform}\left(0, \exp\{\mathcal{L}(\theta_{k-1}) - \frac{1}{2} r_0 \cdot r_0\}\right)$$.
    - Initialise Tree:
        - Set $$\theta^- = \theta-{k-1}, \theta^+ = \theta_{k-1}, r^- = r_0, r^+ = r_0, j = 0$$.
        - Initialise the set of candidate states $$C = \{(\theta-{k-1}, r_0)\}$$.
        - Set the stop-criterion indicator $$s = 1$$.
    - Recursive Tree Building: While $$s = 1$$:
        - Choose a direction $$v_j \sim \text{Uniform}(\{-1, 1\})$$.
        - If $$v_j = -1$$:
            - $$\theta^-, r^-, \_, \_, C', s' \leftarrow \text{BuildTree}(\theta^-, r^-, u, v_j, j, \epsilon)$$
        - Else:
            - $$\_, \_, \theta^+, r^+, C', s' \leftarrow \text{BuildTree}(\theta^+, r^+, u, v_j, j, \epsilon)$$
        - If $$s' = 1$$, then update candidates: $$C \leftarrow C \cup C'$$.
        - Update stop criterion based on the No-U-Turn condition:$$s \leftarrow s' \cdot \mathbb{I}[(\theta^+ - \theta^-) \cdot r^- \ge 0] \cdot \mathbb{I}[(\theta^+ - \theta^-) \cdot r^+ \ge 0]$$
        - $$j \leftarrow j + 1$$.
    - Transition: Sample $$\theta_k$$ uniformly at random from the set of valid states $$C$$.
>
___BuildTree Function___
- Takes in $$\theta$$, $$r$$, $$u$$, $$v$$, $$j$$, and $$\epsilon$$:
- Base Case ($$j = 0$$):
    - Take one leapfrog step in direction $$v$$: $$\theta', r' \leftarrow \text{Leapfrog}(\theta, r, v\epsilon)$$.
    - Define the set $$C'$$ as $$\{(\theta', r')\}$$ if $$u \le \exp\{\mathcal{L}(\theta') - \frac{1}{2} r' \cdot r'\}$$, otherwise $$C' = \emptyset$$. 
    - Set $$s' = \mathbb{I}[u < \exp\{\Delta_{max} + \mathcal{L}(\theta') - \frac{1}{2} r' \cdot r'\}]$$.
    - Return $$\theta', r', \theta', r', C', s'$$.
- Recursion ($$j > 0$$):
    - Build the first subtree: $$\theta^-, r^-, \theta^+, r^+, C', s' \leftarrow \text{BuildTree}(\theta, r, u, v, j-1, \epsilon)$$.
    - If $$s' = 1$$:
        - If $$v = -1$$:
            - $$\theta^-, r^-, \_, \_, C'', s'' \leftarrow \text{BuildTree}(\theta^-, r^-, u, v, j-1, \epsilon)$$
        - Else:
            - $$\_, \_, \theta^+, r^+, C'', s'' \leftarrow \text{BuildTree}(\theta^+, r^+, u, v, j-1, \epsilon)$$
        - Update stop criterion: $$s' \leftarrow s'' \cdot \mathbb{I}[(\theta^+ - \theta^-) \cdot r^- \ge 0] \cdot \mathbb{I}[(\theta^+ - \theta^-) \cdot r^+ \ge 0]$$
        - Update candidates: $$C' \leftarrow C' \cup C''$$.
    - Return $$\theta^-, r^-, \theta^+, r^+, C', s'$$.

<br>

Some things to note:
- We have set the mass matrix to the identity
- The method makes use of [Slice sampling](https://en.wikipedia.org/wiki/Slice_sampling) (with the slice variable being $$u$$)
- The tree is constructing until either the sampler starts retracing steps (satisfying the scalar product condition which is quantified by indicator functions e.g. $$ \mathbb{I}[(\theta^+ - \theta^-) \cdot r^- \ge 0]$$) or one of the trajectories has ventured into a very low probability area
- We now no longer simply take the last sample in the HMC trajectory but rather uniformly sample from the constructed set $$C$$ which holds all the points in the trajectory if it contains valid slice sampling proposals i.e. the condition $$u \le \exp\{\mathcal{L}(\theta') - \frac{1}{2} r' \cdot r'\}$$

Now again, seems great, but the algorithm requires $$2^j-1$$ evaluations of $$\mathcal{L}(\theta)$$ and its gradient (where $$j$$ is the number of times BuildTree() is called), and $$\mathcal{O}(2^j)$$ additional operations to determine when to stop doubling _and_ it requires us to store $$2^j$$ position and momentum coordinates ... this is not great. But for a simple problem such as a gaussian we can implement this idea fine, this is shown in the GIF below.


<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2026-01-LMC/naive_nuts_visualization.gif" 
      alt="naive_nuts_visualization.gif"
      title="naive_nuts_visualization.gif"
      style="width: 100%; height: auto; border-radius: 0px;">
</div>
<br>


### Don't be naive: Updating the transition kernel to make NUTS more efficient

We want to reduce the computational cost incurred above, but there are also better transition kernels and other improvements we can make. 

Other kenels can make larger jumps on $$C$$ so we can explore the probability space more efficiently compared to uniform sampling. 

We can also further reduce some waste in the construction of $$C'$$, because even if $$s' = 0$$ in the middle of a given iteration of doubling, we continue that given iteration anyways despite there being no point as proposals after that point will not be used/are wasted samples. I should have addressed this in the first algorithm, just requires checking the condition and exiting during the relevant part of the procedure, but I'm kinda following the order that [Hoffman and Gelman](https://arxiv.org/abs/1111.4246) did for their paper and I don't want to accidentally skip something important.

To fix the issue of storing so many coodinates [Hoffman and Gelman](https://arxiv.org/abs/1111.4246) suggest the following transition kernel which instead of having to store $$\mathcal{O}(2^j)$$ only has to store $$\mathcal{O}(j)$$.

$$\begin{align}
T(z'\vert z, C) = \begin{cases}
\frac{\mathbb{I}[z' \in C^{new}]}{\vert C^{new}\vert} & \textrm{if} \vert C^{new}\vert > \vert C^{old}\vert \\
\frac{\vert C^{new}\vert}{\vert C^{old}\vert} \frac{\mathbb{I}[z' \in C^{new}]}{\vert C^{new}\vert} + \left(1 - \frac{\vert C^{new}\vert}{\vert C^{old}\vert} \right) \mathbb{I}[z'=z] & \textrm{if} \vert C^{new}\vert \leq \vert C^{old}\vert \\
\end{cases}
\end{align}$$




## I must be NUTS: Adaptive tuning for $$\epsilon$$

I was going to do this but I'm already well outside the scope of what I was planning to do with the HMC portion of this post so I'd just recommend having a look at the section "Adaptively Tunring $$\epsilon$$" in [Hoffman and Gelman](https://arxiv.org/abs/1111.4246) for this. If you are reading this and really want me to cover it shoot me an email at "[firstname]c[lastname]@[google address].com".


# LMC in detail






# Recipes for Stochastic Gradient MCMC






# Exploring Methods discussed in Ma et al. (2015)






