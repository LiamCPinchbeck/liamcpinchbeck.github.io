---
title: 'I <3 Kernels'
date: 2025-11-09
permalink: /posts/2025/11/2025-11-16-i-<3-kernels/
tags:
  - Introductory
  - Dimensional Reduction
  - Machine Learning
  - Bayesian Inference
  - Gaussian Processes
  - Support Vector Machines
  - Regression
header-includes:
   - \usepackage{amsmath}
---

In this post I'm going to go through the kernel trick and how it helps or enables various tools in statistics and machine learning including support vector machines, gaussian processes, kernel regression and kernel PCA. This is going to be a bit of a long one, I'll probably split it up later but for now ... sorry?


## Table of Contents

- [Linear Methods](#linear-methods)
    - [Linear Discriminatory Analysis (KDA I)](#linear-discriminatory-analysis-kda-i)
    - [Support Models (SVM I)](#support-models-svm-i)
    - [Ridge Regression (KRR I)](#ridge-regression-krr-i)
    - [Principle Component Analysis (Kernel PCA I)](#principle-component-analysis-kernel-pca-i)
- [Kernel Trick](#the-kernel-trick)
- [Awesome Kernel-Based Methods](#awesome-kernel-based-methods)
    - [Kernel Discriminatory Analysis (KDA II)](#kernel-discriminatory-analysis-kda-ii)
    - [Support Vector Machines (SVM II)](#support-vector-machines-svm-ii)
    - [Kernel Ridge Regression (KRR II)](#kernel-ridge-regression-krr-ii)
    - [Kernel Principle Component Analysis (Kernel PCA II)](#kernel-principle-component-analysis-kernel-pca-ii)
- [Gallery of Kernels](#gallery-of-kernels)
- [Gaussian Processes](#gaussian-processes)
- [Conclusions / Pros and Cons of Kernel Methods](#conclusions--pros-and-cons-of-kernel-methods)


## General Resources

- [Cambridge series in statistical and probabilistic mathematics: Asymptotic statistics series number 3](https://www.cambridge.org/core/books/asymptotic-statistics/A3C7DAD3F7E66A1FA60E9C8FE132EE1D)
    - Sorry that this one isn't open access, but I was lucky enough to have access through my institution and it's great. If you have institutional access or can afford it I'd highly recommend it (as of 2025)


# Intro

The goal of this blog post is to make you the reader aware or appreciate more kernel-based methods. For that I'm going to structure the post as 1. Some linear-ish methods that show promise for being even better in non-linear contexts but seem computationally expensive, 2. How the kernel trick allows us to get around this computational bottlenecks, and finally 3. The final form of the kernel-based methods and those that simply don't work without it (GPs). 

I'm not going to presume knowledge of these methods beforehand as much as possible, but I am going to do a bit of a whirlwind tour, so if any of them seem interesting to you and you don't think the level of detail I provide is good enough, I've tried to include some independent resources for each that maybe will provide another perspective or more detail for every sub-section (the 'Resources' sections).

And as should be stated in all of my posts (but to be clear isn't), I use notation through which I understand everything or simply want consistent notation throughout a given post so will likely differ from standard notation for a given topic(s). If you think I should make a given idea or object different notation either to make it clearer or because it's simply incorrect please email me at lc[LastNamelowerCase]@[googleAddress].com or [FirstName].[Lastname]@[my institution].edu


# Linear Methods

## Fisher Linear Discriminatory Analysis (KDA I)

### Resources
- [StatQuest: Linear Discriminant Analysis (LDA) clearly explained.](https://www.youtube.com/watch?v=azXCzI57Yfc)
- [Linear discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) - Wikipedia
- [Basics of Quadratic Discriminant Analysis (QDA)](https://www.kaggle.com/discussions/general/448328) - Kaggle
- [FISHER'S DISCRIMINANT ANALYSIS](https://www.youtube.com/watch?v=74QFmqHOQcU) - [Sanjoy Das](https://www.youtube.com/@SanjoyDasVideos)
- [The Use Of Multiple Measurements In Taxonomic Problems](https://digital.library.adelaide.edu.au/server/api/core/bitstreams/1801cd68-028a-4380-a9c6-30ca9b0aa0d3/content) - Fisher
- [Kernel Fisher discriminant analysis](https://en.wikipedia.org/wiki/Kernel_Fisher_discriminant_analysis)
    -  Has a nice concise section on the linear case

### The Gist

Fisher Linear Discriminatory Analysis or simply FLDA[^FLDAvsLDA] is a supervised method (meaning we know the class labels) for creating a linear projection (single value in 1D, line in 2D, plane in 3D) that separates two or more classes of objects. Here we will focus on the separation of just two classes.

[^FLDAvsLDA]: It annoys me to no end that Fisher Linear Discriminatory Analysis and Linear Discriminatory Analysis are commonly used interchangeably. Strictly "Linear Discriminatory Analysis" assumes homoscedacity (same covariances) between the two groups and that they follow normal distributions. It is for this reason that I gave up on finding a probabilistic derivation of FLDA, and I ain't spending the time deriving it myself. Kernel Discriminatory Analysis as far as I can see is based on Fisher LDA, hence I focus on that.

The key idea behind FLDA is that you wish to construct some linear combination of the input variables to demarcate the two classes that you project the objects onto. You do this by 1. maximising the distance or _variance_ between the two groups on the projected space __and__ 2. minimising the variance of each group in this space. Below are some examples of this in action before we get into it to emphasise that both conditions must be satisfied to get good discrimination.


<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/EDA.png" 
      alt="Nothing to see here." 
      title="Nothing to see here." 
      style="width: 99%; height: auto; border-radius: 0px;">
</div>


In a very non-statiscian way I'm just going to throw the formula here and leave the derivation to another day (as I will do for quite a few things in this post).

$$\begin{align}
Z = \frac{\sigma^2_{\textrm{inbetween}}}{\sigma^2_{\textrm{within}}} = \frac{(\vec{w}\cdot(\vec{\mu}_1 - \vec{\mu}_0))^2}{\vec{w}^T\left(\Sigma_0 + \Sigma_1\right)\vec{w}}
\end{align}$$


The $$\vec{w}$$ is a vector in the direction of line (or linear operator on variables for generality) that is used in the above as a projection operator, to note the statistical measures on the line. There is any analytical solution to this where,

$$\begin{align}
\vec{w} \propto (\Sigma_0 + \Sigma_1)^{-1}(\vec{\mu}_1 - \vec{\mu}_0).
\end{align}$$

Where the solution is proportional too as increases or decreases in the magnitude of the direction vector still return the same line object. For the sake of a cool gif and for later on when we generalise this method, let's look at how it looks when you try to optimise for $$\vec{w}$$ and compare it to the exact solution above.

<br>

Let's compare the optimisation result to my "guesses" above.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/LDA_progression.gif" 
      alt="GIF Showing Progression of LDA Optimisation" 
      title="GIF Showing Progression of LDA Optimisation" 
      style="width: 99%; height: auto; border-radius: 0px;">
</div>

<br>

And voila, my good guess wasn't as good as I thought and my bad guesses were in fact ... bad.

Switching gears a little bit, let's simplify the problem and look at a 1D example, still 2 groups.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/non_linear_qda_data.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 90%; height: auto; border-radius: 0px;">
</div>

<br>


Now, if we wanted to do FLDA, we can only create a single value to discriminate the groups. But no matter what value you pick, there is no single value that will perfectly discriminate the groups despite it visually looking very simple.

The trick is to[^sneaky] increase the dimension of the space, projecting the 1D dimensional data $$x$$ into two dimensions where the second is $$x^2$$. 

[^sneaky]: Or as my favourite math teacher Mr D'Amico used to say, "We're gonna do something _sneaky_ and ..."

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/quadratic_lda_demarcation.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 79%; height: auto; border-radius: 0px;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/Quadratic_Separation_in_LDA_Subspace.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 89%; height: auto; border-radius: 0px;">
</div>

<br>
In this space, it is an extremely simple task of constructing a linear combination of the variables (again just a line in 2D) that the data can be projected onto and be separated. Additionally, I'll start plotting the _decision boundary_ that this implies in the space, which both separates the data and shows what direction it needs to go to be projected onto the LDA space.

Mathematically, nothing really changes compared to before, but we can slightly rephrase it so that it's easier to generalise to more dimensions.

So we go back to maximising the variance $$B$$etween the class and $$W$$ithin for data from group 0, $$x_0$$, and group 1 $$x_1$$.

$$\begin{align}
T &= \frac{w^T S_B w}{w^T S_W w} \\
&= \frac{\hat{x_0} + \hat{x_1}}{\sum_i (x_0^i - \hat{x_0})^T(x_0^i - \hat{x_0}) + \sum_j (x_1^j - \hat{x_1})^T(x_1^j - \hat{x_1})}
\end{align}$$




Okay great now we can separate groups like that. But let's add one more cluster of points, and project it into the same space

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/double_double_set_data.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/double_double_set_data_quadratic_projection.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
      
</div>


The projection doesn't really help us again, no matter what line you pick you can't separate the groups. But what if we keep going? Projecting it into the 3D space with the third dimension being $$x^3$$ gives us the plot below (interactive).


<iframe 
    src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/3d_scatter_with_projection_line.html" 
    width="100%" 
    height="600px"
    title="Embedded Content"
    style="border:none;"
></iframe>


Where we've constructed the lina and planar boundary which are just linear combinations of the variables we are considering $$x^3$$, $$x^2$$ and $$x^1$$. I like to think of the plane here as the surface over which the samples radially converge onto the LDA projection.

You can see that increasing the dimensionality allows us to separate groups with more complicated morphologies but the computational cost quickly explodes in the case of even low dimensional data. e.g. If our data is just two dimensional $$x$$ and $$y$$ then the "cubic" projected space would contain $$[1, x, y, xy, x^2, y^2, xy^2, x^2y, x^3, y^3]$$ (with the 1 included to highlight how the lower dimensional contributions stick around). So we went from $$\mathbb{R}^2$$ to $$\mathbb{R}^9$$ (excluding 1) and you can imagine how bad this would get in the case of 4D data like $$x, y, z$$ and time, and a degree 4 polynomial projection ... but note that all the optimisation and the projections only go through dot products ...



## Support Models (SVM I)

### Resources

- [Support Vector Machines Part 1 (of 3): Main Ideas!!!](https://www.youtube.com/watch?v=efR1C6CvhmE&t=60s)
- [16. Learning: Support Vector Machines](https://www.youtube.com/watch?v=_PwhiWxHK8o) - [MIT OCW](https://www.youtube.com/@mitocw)

## The Gist

Support vector machines are another supervised linear method where we want to construct some way to separate data, except unlike FLDA where we want to develop the projection, instead we want to figure out the boundary, then we can figure out the projection later if we want. I'm just gonna come up and say that I'm stealing most of my examples from the MIT OCW lecture above. 

The use case is similar to before except I want to raise two particular very similar issues in FLDA. Let's say we have the two datasets below.


<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_case.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_case_2.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
</div>

Using FLDA we would construct boundaries similar to the below, and we want to introduce some new data that is unlabelled.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_case_2_w_bad_line.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_case_w_bad_line.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
</div>

In both cases, it seems obvious to us what the new points should be labelled as, but using two lines that are perfectly fine under FLDA, both would be mis-characterised. The solution, is to 1. Maximise the region around the boundary and 2. not care so much about mis-labelling some points in our training data. Using these principles you can imagine that we would get something more reasonable.


<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_case_2_w_good_line.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_case_w_good_line.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
</div>

To me, this seems much more reasonable. And you might have noticed that I've highlighted a couple points, reason being that the decision boundary that is constructed here is basically just the average position of these two points. Removing any other points wouldn't make the boundary any better or worse (except maybe the blue point in the right example). We call these points _support vectors_ and it's where the name for _Support Vector Machines_ come from. I'll circle back to _why_ this is in a bit.

Before jumping into Support Vector Machines, instead I want to start with Support Vector _Models_ and how they are constructed. (From here I'm pretty closely "following" the MIT OCW lecture.)





## Ridge Regression (KRR I)

### Resources



## Principle Component Analysis (Kernel PCA I)

### Resources





# The Kernel Trick

## Resources






# Awesome Kernel-Based Methods

## Kernel Discriminatory Analysis (KDA II)

### Resources

- [Kernel Fisher discriminant analysis](https://en.wikipedia.org/wiki/Kernel_Fisher_discriminant_analysis)



## Support Vector Machines (SVM II)

### Resources





## Kernel Ridge Regression (KRR II)

### Resources





## Kernel Principle Component Analysis (Kernel PCA II)

### Resources









# Gallery of Kernels

## Resources








# Gaussian Processes

## Resources

- [A Practical Guide to Gaussian Processes](https://infallible-thompson-49de36.netlify.app/)
- [Interactive Gaussian Process Visualization](https://www.infinitecuriosity.org/vizgp/)
- [Gaussian process regression demo](https://www.tmpl.fi/gp/)
- [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/chapters/RW.pdf)
















# Conclusions / Pros and Cons of Kernel Methods









