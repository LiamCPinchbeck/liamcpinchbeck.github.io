---
title: 'I <3 Kernels'
date: 2025-11-09
permalink: /posts/2025/11/2025-11-16-i-<3-kernels/
tags:
  - Introductory
  - Dimensional Reduction
  - Machine Learning
  - Bayesian Inference
  - Gaussian Processes
  - Support Vector Machines
  - Regression
header-includes:
   - \usepackage{amsmath}
---

In this post I'm going to go through the kernel trick and how it helps or enables various tools in statistics and machine learning including support vector machines, gaussian processes, kernel regression and kernel PCA. This is going to be a bit of a long one, I'll probably split it up later but for now ... sorry?


## Table of Contents

- [Linear Methods](#linear-methods)
    - [Fisher Linear Discriminatory Analysis (KDA I)](#fisher-linear-discriminatory-analysis-kda-i)
    - [Support Models (SVM I)](#support-models-svm-i)
    - [Ridge Regression (KRR I)](#ridge-regression-krr-i)
    - [Principle Component Analysis (Kernel PCA I)](#principle-component-analysis-kernel-pca-i)
- [Kernel Trick](#the-kernel-trick)
- [Awesome Kernel-Based Methods](#awesome-kernel-based-methods)
    - [Kernel Discriminatory Analysis (KDA II)](#kernel-discriminatory-analysis-kda-ii)
    - [Support Vector Machines (SVM II)](#support-vector-machines-svm-ii)
    - [Kernel Ridge Regression (KRR II)](#kernel-ridge-regression-krr-ii)
    - [Kernel Principle Component Analysis (Kernel PCA II)](#kernel-principle-component-analysis-kernel-pca-ii)
- [Gallery of Kernels](#gallery-of-kernels)
- [Gaussian Processes](#gaussian-processes)
- [Conclusions / Pros and Cons of Kernel Methods](#conclusions--pros-and-cons-of-kernel-methods)

# Prerequisites

Sorry in advance if this post isn't as accessible as my other ones. Since it's quite a long one I'm going to presume some knowledge going in. Primarily,

- [dot products](https://en.wikipedia.org/wiki/Dot_product) + [inner products](https://en.wikipedia.org/wiki/Inner_product_space) and that one is a subset of the other
- [Projections on to lines and planes from a Linear Algebra perspective](https://en.wikipedia.org/wiki/Projection_(linear_algebra))
- [Lagrange Multiplier problems](https://en.wikipedia.org/wiki/Lagrange_multiplier)
- and as usual, basic calculus (primarily multivariable derivatives of and with scalars and vectors) and some matrix algebra.

And yes all the links are Wikipedia, they're good pages. And if Wikipedia is there, why not use it.

## General Resources

- [Cambridge series in statistical and probabilistic mathematics: Asymptotic statistics series number 3](https://www.cambridge.org/core/books/asymptotic-statistics/A3C7DAD3F7E66A1FA60E9C8FE132EE1D)
    - Sorry that this one isn't open access, but I was lucky enough to have access through my institution and it's great. If you have institutional access or can afford it I'd highly recommend it (as of 2025)
- [High-Dimensional Statistics - A Non-Asymptotic Viewpoint](https://www.cambridge.org/core/books/highdimensional-statistics/8A91ECEEC38F46DAB53E9FF8757C7A4E) - Martin J. Wainwright
    - Also sorry that this one isn't open access either, same situation as above.
- [Jeff Calder: "An intro to concentration of measure with applications to graph-based l... (Part 1/2)"](https://youtu.be/Q5fB5Ldzo-g) - Institute for Pure & Applied Mathematics (IPAM)
    - Not directly relevant, but is something to think about with all the talk of high-dimensional statistics on this page


# Intro

The goal of this blog post is to make you the reader aware or appreciate more kernel-based methods. For that I'm going to structure the post as 1. Some linear-ish methods that show promise for being even better in non-linear contexts but seem computationally expensive, 2. How the kernel trick allows us to get around this computational bottlenecks, and finally 3. The final form of the kernel-based methods and those that simply don't work without it (GPs). 

I'm not going to presume knowledge of these methods beforehand as much as possible, but I am going to do a bit of a whirlwind tour, so if any of them seem interesting to you and you don't think the level of detail I provide is good enough, I've tried to include some independent resources for each that maybe will provide another perspective or more detail for every sub-section (the 'Resources' sections).

And as should be stated in all of my posts (but to be clear isn't), I use notation through which I understand everything or simply want consistent notation throughout a given post so will likely differ from standard notation for a given topic(s). If you think I should make a given idea or object different notation either to make it clearer or because it's simply incorrect please email me at lc[LastNamelowerCase]@[googleAddress].com or [FirstName].[Lastname]@[my institution].edu


# Linear Methods

## Fisher Linear Discriminatory Analysis (KDA I)

### Resources
- [StatQuest: Linear Discriminant Analysis (LDA) clearly explained.](https://www.youtube.com/watch?v=azXCzI57Yfc)
- [Linear discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) - Wikipedia
- [Basics of Quadratic Discriminant Analysis (QDA)](https://www.kaggle.com/discussions/general/448328) - Kaggle
- [FISHER'S DISCRIMINANT ANALYSIS](https://www.youtube.com/watch?v=74QFmqHOQcU) - [Sanjoy Das](https://www.youtube.com/@SanjoyDasVideos)
- [The Use Of Multiple Measurements In Taxonomic Problems](https://digital.library.adelaide.edu.au/server/api/core/bitstreams/1801cd68-028a-4380-a9c6-30ca9b0aa0d3/content) - Fisher
- [Kernel Fisher discriminant analysis](https://en.wikipedia.org/wiki/Kernel_Fisher_discriminant_analysis)
    -  Has a nice concise section on the linear case

### The Gist

Fisher Linear Discriminatory Analysis or simply FLDA[^FLDAvsLDA] is a supervised method (meaning we know the class labels) for creating a linear projection (single value in 1D, line in 2D, plane in 3D) that separates two or more classes of objects. Here we will focus on the separation of just two classes.

[^FLDAvsLDA]: It annoys me to no end that Fisher Linear Discriminatory Analysis and Linear Discriminatory Analysis are commonly used interchangeably. Strictly "Linear Discriminatory Analysis" assumes homoscedacity (same covariances) between the two groups and that they follow normal distributions. It is for this reason that I gave up on finding a probabilistic derivation of FLDA, and I ain't spending the time deriving it myself. Kernel Discriminatory Analysis as far as I can see is based on Fisher LDA, hence I focus on that.

The key idea behind FLDA is that you wish to construct some linear combination of the input variables to demarcate the two classes that you project the objects onto. You do this by 1. maximising the distance or _variance_ between the two groups on the projected space __and__ 2. minimising the variance of each group in this space. Below are some examples of this in action before we get into it to emphasise that both conditions must be satisfied to get good discrimination.


<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/EDA.png" 
      alt="Nothing to see here." 
      title="Nothing to see here." 
      style="width: 99%; height: auto; border-radius: 0px;">
</div>


In a very non-statiscian way I'm just going to throw the formula here and leave the derivation to another day (as I will do for quite a few things in this post).

$$\begin{align}
Z = \frac{\sigma^2_{\textrm{inbetween}}}{\sigma^2_{\textrm{within}}} = \frac{(\vec{w}\cdot(\vec{\mu}_1 - \vec{\mu}_0))^2}{\vec{w}^T\left(\Sigma_0 + \Sigma_1\right)\vec{w}}
\end{align}$$


The $$\vec{w}$$ is a vector in the direction of line (or linear operator on variables for generality) that is used in the above as a projection operator, to note the statistical measures on the line. There is any analytical solution to this where,

$$\begin{align}
\vec{w} \propto (\Sigma_0 + \Sigma_1)^{-1}(\vec{\mu}_1 - \vec{\mu}_0).
\end{align}$$

Where the solution is proportional too as increases or decreases in the magnitude of the direction vector still return the same line object. For the sake of a cool gif and for later on when we generalise this method, let's look at how it looks when you try to optimise for $$\vec{w}$$ and compare it to the exact solution above.

<br>

Let's compare the optimisation result to my "guesses" above.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/LDA_progression.gif" 
      alt="GIF Showing Progression of LDA Optimisation" 
      title="GIF Showing Progression of LDA Optimisation" 
      style="width: 99%; height: auto; border-radius: 0px;">
</div>

<br>

And voila, my good guess wasn't as good as I thought and my bad guesses were in fact ... bad.

Switching gears a little bit, let's simplify the problem and look at a 1D example, still 2 groups.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/non_linear_qda_data.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 90%; height: auto; border-radius: 0px;">
</div>

<br>


Now, if we wanted to do FLDA, we can only create a single value to discriminate the groups. But no matter what value you pick, there is no single value that will perfectly discriminate the groups despite it visually looking very simple.

The trick is to[^sneaky] increase the dimension of the space, projecting the 1D dimensional data $$x$$ into two dimensions where the second is $$x^2$$. 

[^sneaky]: Or as my favourite math teacher Mr D'Amico used to say, "We're gonna do something _sneaky_ and ..."

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/quadratic_lda_demarcation.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 79%; height: auto; border-radius: 0px;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/Quadratic_Separation_in_LDA_Subspace.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 89%; height: auto; border-radius: 0px;">
</div>

<br>
In this space, it is an extremely simple task of constructing a linear combination of the variables (again just a line in 2D) that the data can be projected onto and be separated. Additionally, I'll start plotting the _decision boundary_ that this implies in the space, which both separates the data and shows what direction it needs to go to be projected onto the LDA space.

Mathematically, nothing really changes compared to before, but we can slightly rephrase it so that it's easier to generalise to more dimensions.

So we go back to maximising the variance $$B$$etween the class and $$W$$ithin for data from group 0, $$x_0$$, and group 1 $$x_1$$.

$$\begin{align}
T &= \frac{w^T S_B w}{w^T S_W w} \\
&= \frac{\hat{x_0} + \hat{x_1}}{\sum_i (x_0^i - \hat{x_0})^T(x_0^i - \hat{x_0}) + \sum_j (x_1^j - \hat{x_1})^T(x_1^j - \hat{x_1})}
\end{align}$$




Okay great now we can separate groups like that. But let's add one more cluster of points, and project it into the same space

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/double_double_set_data.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/double_double_set_data_quadratic_projection.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
      
</div>


The projection doesn't really help us again, no matter what line you pick you can't separate the groups. But what if we keep going? Projecting it into the 3D space with the third dimension being $$x^3$$ gives us the plot below (interactive).


<iframe 
    src="/files/BlogPostData/2025-i-<3-kernels/LDA_Figures/3d_scatter_with_projection_line.html" 
    width="100%" 
    height="600px"
    title="Embedded Content"
    style="border:none;"
></iframe>


Where we've constructed the lina and planar boundary which are just linear combinations of the variables we are considering $$x^3$$, $$x^2$$ and $$x^1$$. I like to think of the plane here as the surface over which the samples radially converge onto the LDA projection.

You can see that increasing the dimensionality allows us to separate groups with more complicated morphologies, or to handle morphologies were we don't know what projcetion will work a priori, but the computational cost quickly explodes in the case of even low dimensional data. e.g. If our data is just two dimensional $$x$$ and $$y$$ then the "cubic" projected space would contain $$[1, x, y, xy, x^2, y^2, xy^2, x^2y, x^3, y^3]$$ (with the 1 included to highlight how the lower dimensional contributions stick around). So we went from $$\mathbb{R}^2$$ to $$\mathbb{R}^9$$ (excluding 1) and you can imagine how bad this would get in the case of 4D data like $$x, y, z$$ and time, and a degree 4 polynomial projection ... but note that all the optimisation and the projections only go through dot products ...

Here's the code for calculating the FLDA projection cost function for an arbitrary set of samples and dimensionality.

```python
def calculate_scatter_matrices(samples1: np.ndarray, samples2: np.ndarray):
    mean1 = np.mean(samples1, axis=0)
    mean2 = np.mean(samples2, axis=0)
    
    N1 = samples1.shape[0]
    N2 = samples2.shape[0]
    N_total = N1 + N2
    total_mean = (N1 * mean1 + N2 * mean2) / N_total

    centered1 = samples1 - mean1
    S1 = np.dot(centered1.T, centered1) 
    
    centered2 = samples2 - mean2
    S2 = np.dot(centered2.T, centered2) 
    S_W = S1 + S2
    
    diff_mean1 = (mean1 - total_mean)[:, np.newaxis]
    diff_mean2 = (mean2 - total_mean)[:, np.newaxis]
    S_B = N1 * np.dot(diff_mean1, diff_mean1.T) + N2 * np.dot(diff_mean2, diff_mean2.T)

    return S_W, S_B


def lda_objective_function(w, S_W, S_B):
    
    numerator = w.T @ S_B @ w
    
    denominator = w.T @ S_W @ w
    
    J_w = numerator / denominator

    
    return -J_w
```


## Support Models (SVM I)

### Resources

- [Support Vector Machines Part 1 (of 3): Main Ideas!!!](https://www.youtube.com/watch?v=efR1C6CvhmE&t=60s)
- [16. Learning: Support Vector Machines](https://www.youtube.com/watch?v=_PwhiWxHK8o) - [MIT OCW](https://www.youtube.com/@mitocw)

### The Gist

Support vector machines are another supervised linear method where we want to construct some way to separate data, except unlike FLDA where we want to develop the projection, instead we want to figure out the boundary, then we can figure out the projection later if we want. I'm just gonna come up and say that I'm stealing most of my examples from the MIT OCW lecture above. 

The use case is similar to before except I want to raise two particular very similar issues in FLDA. Let's say we have the two datasets below.


<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_case.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_case_2.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
</div>

Using FLDA we would construct boundaries similar to the below, and we want to introduce some new data that is unlabelled.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_case_2_w_bad_line.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_case_w_bad_line.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
</div>

In both cases, it seems obvious to us what the new points should be labelled as, but using two lines that are perfectly fine under FLDA, both would be mis-characterised. The solution, is to 1. Maximise the region around the boundary and 2. not care so much about mis-labelling some points in our training data. Using these principles you can imagine that we would get something more reasonable.


<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_case_2_w_good_line.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_case_w_good_line.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
</div>

To me, this seems much more reasonable. And you might have noticed that I've highlighted a couple points, reason being that the decision boundary that is constructed here is basically just the average position of these two points. Removing any other points wouldn't make the boundary any better or worse (except maybe the blue point in the right example). We call these points _support vectors_ and it's where the name for _Support Vector Machines_ come from. I'll circle back to _why_ this is in a bit.

Before jumping into Support Vector Machines, instead I want to start with Support Vector _Models_ and how they are constructed. (From here I'm pretty closely "following" the MIT OCW lecture.)

To make the visualisations less boring let's go back to 2D. I spent too much time coming up with some useful randomish values.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_2D_case.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 79%; height: auto; border-radius: 0px;">
</div>

If we wanted to create a boundary that is maximally wide that distinguishes the two classes it would look something like this.


<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_2D_case_boundary_guess.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 79%; height: auto; border-radius: 0px;">
</div>

The region would then be defined either by the direction vector of the orange/yellow dashed line or the normal vector. For now let's say we want to use a normal, the length doesn't matter, just the direction. This then let's us very nicely define whether a point $$\vec{x}$$ is contained in the region by a projection onto this line $$\vec{w}\cdot\vec{x} + b$$. We define the yellow line to be the 0 point on the projection $$\vec{w}\cdot\vec{x} + b = 0$$, and the boundaries to correspond to $$\pm1$$, $$\vec{w}\cdot\vec{x} + b = \pm 1$$.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_2D_case_boundary_guess_w_dirvec.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 99%; height: auto; border-radius: 0px;">
</div>

The area above the region will then be larger than $$1$$, $$\vec{w}\cdot\vec{x} + b > 1$$, and below will be less than $$-1$$, $$\vec{w}\cdot\vec{x} + b < -1$$.

----

Okay, so we have the kind of goal we want, the question is how do we mathematically recover it (can't just do it 'by' eye in 4D, or 16D for that matter). Let's outline what we know.

We know for points from group 1/orange points, that they evaluate to larger than or equal to $$1$$,

$$\begin{align}
\vec{w} \cdot \vec{x}_1 + b \geq 1,
\end{align}$$

and for points from group 2/blue points, that they evaluate to less than or equal to $$-1$$,

$$\begin{align}
\vec{w} \cdot \vec{x}_2 + b \leq -1.
\end{align}$$

Keeping track of two separate expressions like this is kind of annoying though. So let's introduce a mathematical 'nicety' where we label points from group 1 with $$+1$$ and those from group 2 with $$-1$$. Hence, for all points ($$\forall i\in [1, N]$$ $$N$$ being the total number of points),

$$\begin{align}
y_i(\vec{w} \cdot \vec{x}_i + b) \geq 1.
\end{align}$$

That's better! Now, the single mathematical object that we wanted to maximise was the width of the region. 

The width of the region is dictated by the _support vectors_ that I've highlighted with a '+' for group 1 and '-' for group 2. 

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/initial_2D_case_boundary_guess_width_calc.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 99%; height: auto; border-radius: 0px;">
</div>

To figure out the width of the region we just need to project the distance between these two points and that of the perpendicular direction vector defining the region, this is show with a smaller red arrow in the above figure. It's defined by a simple dot product as well (these dot products seem to be showing up a lot in this blog (hint hint)) with the normalised direction vector.

$$\begin{align}
\textrm{Width} = (\vec{x}_+ - \vec{x}_-) \cdot \frac{\vec{w}}{||\vec{w}||}
\end{align}$$

With a bit of algebra, and using the fact that the points lie at the lines at which $$\vec{w}\cdot\vec{x}_+ + b = +1$$ and $$\vec{w}\cdot\vec{x}_- + b = -1$$ to go from line 2 to 3.

$$\begin{align}
\textrm{Width} &= (\vec{x}_+ - \vec{x}_-) \cdot \frac{\vec{w}}{||\vec{w}||} \\
&= \frac{1}{||\vec{w}||} \left( \vec{x}_+ \cdot \vec{w} - \vec{x}_- \cdot  \vec{w} \right)\\
&= \frac{1}{||\vec{w}||} \left( (1-b) - (-1-b) \right)\\
&= \frac{2}{||\vec{w}||}
\end{align}$$

So the goal is to maximise $$\frac{2}{\vert\vert\vec{w}\vert\vert}$$, which is equivalent to maximising $$\frac{1}{\vert\vert\vec{w}\vert\vert}$$, which is equivalent to minimising $$\vert\vert\vec{w}\vert\vert$$, which is finally equivalent to minimising $$\vec{w}^2$$, cool![^convexity]. But now instead of us wondering what the width is, we're wondering what $$\vec{w}$$ is...

[^convexity]: From what I can tell this switch up ensures convexity of the problem, meaning that there is always a single local minima and it is the global minima.

From here we view the problem a little differently, we have an object that we wish to minimise, $$w^2$$, subject to a bunch of constraints, $$y_i(\vec{w} \cdot \vec{x}_i + b) \geq 1$$, ... sounds like a [Lagrange Multiplier](https://en.wikipedia.org/wiki/Lagrange_multiplier) problem. Jumping straight into it.

$$\begin{align}
L = {\color{blue} \frac{1}{2} \vec{w}^2} + {\color{red} \sum_i \alpha_i \left(y_i (\vec{w}\cdot \vec{x}_i + b) -1 \right) }
\end{align}$$

In $${\color{blue}\textrm{blue/left term}}$$ is the thing we are minimising, and in $${\color{red}\textrm{red/right term}}$$ are the constraints with $${\color{red} \alpha_i}$$ the set of lagrange multipliers. The cool thing here is that $$\vec{w}$$ is just some linear combination of the points in the two sets! That's it, no weird error functions or logs or exponentials, just a simple linear combination (one might even say dot product).


Then we do the same ol' song and dance. Taking derivatives with respect to $${\color{red} \alpha_i}$$ just gives us back our constraints, so focusing in on $$\vec{w}$$ and $$b$$.

$$\begin{align}
\frac{\partial L}{\partial \vec{w}} = {\color{blue} \vec{w}} + {\color{red} \sum_i \alpha_i y_i \vec{x}_i } = 0 \\
\rightarrow  {\color{green} \vec{w}  = - \sum_i \alpha_i y_i \vec{x}_i}
\end{align}$$


$$\begin{align}
\frac{\partial L}{\partial b} = {\color{red} \sum_i \alpha_i y_i} = 0 \\
\rightarrow {\color{purple} \sum_i \alpha_i y_i = 0 }
\end{align}$$

Highlighting the important bits in green and purple. We can then sub this back into our loss $$L$$/lagrange equation. 

$$\begin{align}
L = \frac{1}{2} &\left({\color{green} - \sum_i \alpha_i y_i \vec{x}_i}\right) \cdot \left({\color{green} - \sum_j \alpha_j y_j \vec{x}_i}\right) \\
&+ \sum_i \alpha_i \left(y_i \left[\left({\color{green} - \sum_j \alpha_j y_i \vec{x}_j}\right)\cdot \vec{x}_i + b\right] -1 \right) \\
= \frac{1}{2} &\left({\color{green} \sum_i \alpha_i y_i \vec{x}_i}\right) \cdot \left({\color{green} \sum_j \alpha_j y_j \vec{x}_j}\right) \\

&- \left({\color{green} \sum_i \alpha_i y_i \vec{x}_i}\right) \cdot \left({\color{green} \sum_j \alpha_j y_j \vec{x}_j}\right)  \\
&+ b{\color{purple} \sum_i \alpha_i y_i} - \sum_i \alpha_i \\
L = \frac{1}{2} &\sum_i \left [ \alpha_i y_i  \left(\sum_j \alpha_j y_j \, \vec{x}_i \cdot\vec{x}_j \right)   - \alpha_i \right]
\end{align}$$

Staying to the theme, the final calculation comes down to a simple dot product between the data points $$\vec{x}_i \cdot\vec{x}_j $$. So unlike FLDA we don't have an analytical solution, but we do have a function that we can optimise. 

The aim of the game is now to optimise over $$\alpha_i$$ to minimise $$L$$. 

Translating this into the code.

```python
def supportvecmodel_costfunc(alphavec, datavecs, labelvec):     
    K = np.dot(datavecs, datavecs.T) # wink wink
    
    alpha_y = alphavec * labelvec
    
    P = np.outer(alpha_y, alpha_y) * K
    quadratic_term = 0.5 * np.sum(P)
    
    linear_term = -np.sum(alphavec)

    cost = quadratic_term + linear_term
    
    return cost
```

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/final_SVM_result_unscaled.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/final_SVM_result_scaled.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 49%; height: auto; border-radius: 0px;">
    <figcaption>Fig: Putting both the unscaled and scaled plots for comparison and to actually see that the lines are actually perpendicular.</figcaption>
</div>

<br>

You can hopefully see that the margins are significantly wider despite not having 100% accuracy when discriminating between the two groups. This is a pretty simple example though, let's see what it looks like when we increase the number of samples.

<br>

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/big_example_SVM_result_scaled.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 60%; height: auto; border-radius: 0px;">
</div>

<br>

We can also plot the weights for how much each point is contributing to the decision boundary for fun, and broadly it makes sense.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/big_example_SVM_result_scaled_weights.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 80%; height: auto; border-radius: 0px;">
</div>

So this is great, but as noted in the previous section, data isn't always separated so nicely. For example...


<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/circular_example_data_guess_1.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 32%; height: auto; border-radius: 0px;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/circular_example_data_guess_3.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 32%; height: auto; border-radius: 0px;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/circular_example_data_guess_2.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 32%; height: auto; border-radius: 0px;">
</div>

<br>

<div style="text-align: center;">
  <img 
      src="https://media1.tenor.com/m/qxIrXMTgwiEAAAAC/what-the-hell-am-i-supposed-to-do-what-do-you-want-me-to-do.gif" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 60%; height: auto; border-radius: 0px;">
</div>

<br>


Well, we can use the exact same trick we used in the FLDA case, we add a dimension which is some polynomial combination of the variables (for those that already know where I'm going, this corresponds to a degree 2 polynomial kernel).

<iframe 
    src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/circular_example_data_projected_correct_surafaces.html" 
    width="100%" 
    height="600px"
    title="Embedded Content"
    style="border:none;"
></iframe>

And to be clear, this uses the exact same math as above, just now $$x^2+y^2$$ is another dimension in $$\vec{x}_i = [x, y, x^2+y^2]$$ with the same ol' dot products. This is great! Right? Well then what about this example?

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/SM_Figures/cant_separate.png" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 60%; height: auto; border-radius: 0px;">
</div>

It might seem a bit contrived but you could imagine a study where they're looking at the interaction of two drugs. A little of both does nothing, a good amount of one and a lil of the other yields good outcomes, but too much of both leads to unwanted drug interactions and bad outcomes. The quadratic projection wouldn't help here, and other low order polynomial projections likely wouldn't either. So once again.

<div style="text-align: center;">
  <img 
      src="https://media1.tenor.com/m/qxIrXMTgwiEAAAAC/what-the-hell-am-i-supposed-to-do-what-do-you-want-me-to-do.gif" 
      alt="It's 10pm, I'm not coming up with a caption" 
      title="It's 10pm, I'm not coming up with a caption" 
      style="width: 60%; height: auto; border-radius: 0px;">
</div>

## Ridge Regression (KRR I)

### Resources

- [Bayesian Linear Regression : Data Science Concepts](https://youtu.be/Z6HGJMUakmc) - by my man [ritvikmath](https://www.youtube.com/@ritvikmath)
- [Data analysis recipes:Fitting a model to data - ArXiV:1008.4686](https://arxiv.org/pdf/1008.4686) - David Hogg, Jo Bovy, Dustin Lang
- [Ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares)
- [Ridge regression](https://en.wikipedia.org/wiki/Ridge_regression)
- [Generalised Kernel Machines](https://www.researchgate.net/publication/221534649_Generalised_Kernel_Machines)
- [Gaussâ€“Markov theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem)
- [Bias-Variance Trade Off](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)
- [Temperature and Ice Cream Sales](https://www.kaggle.com/datasets/raphaelmanayon/temperature-and-ice-cream-sales) Kaggle dataset by [Raphael Manayon](https://www.linkedin.com/in/raphael-manayon-b29444286/)


### The Gist

Switching gears, or more switching cars at this point, let's talk about regression. Plain ol' linear regression. You have a dependent variable, $$y$$, an independent variable $$x$$, a set of samples of each $$\{y_i\}$$ and $$\{x_i\}$$, and you believe they are related via $$y= \alpha_1 x + \alpha_0$$ (excuse the pointlessly mathy way of describing things, it will be useful later).

This is exactly what I did in my [first blog post](https://liamcpinchbeck.github.io/posts/2025/01/2025-01-26-first-blog-post/) but in that case I went straight into the Bayesian probabilistic route. If we just wanted a best fit however, we don't need to go through all the effort. 

We can just define a linear system of matrices. Defining,

$$\begin{align}
Y = \left[\begin{matrix}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{matrix} \right]
\end{align}$$

$$\begin{align}
X = \left[ 
\begin{matrix}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_N \\
\end{matrix}
\right]
\end{align}$$


$$\begin{align}
\vec{\alpha} = \left[\begin{matrix}
\alpha_0 & \alpha_1 
\end{matrix}
\right]
\end{align}$$

Using the above[^nonstandard] we can define the system as,

[^nonstandard]: I'm using relatively non-standard notation here, but I genuinely hate putting the $$x$$ values in the $$A$$ matrix and the $$\alpha$$ values in the X matrix.

$$\begin{align}
Y = X \vec{\alpha}.
\end{align}$$

And that's it. $$Y$$ is called the ___regressand___, and $$X$$ the ___design matrix___. The aim of the game is to just figure out $$\vec{\alpha}$$ ... but wait, sure I don't need uncertainties on my final parameter values, but I should include the uncertainties on my data if I have them right?? That doesn't show up in the above. Well for that I find it easier to actually go back to the probabilities.

If we're assuming that our data is distributed according to some normal distribution like the following (assuming [homoscedacity](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity) for simplicity),

$$
\begin{align}
y_i \sim \mathcal{N}(\alpha_1 x_i + \alpha_0, \sigma^2),
\end{align}
$$

then our system becomes,

$$\begin{align}
Y = X \vec{\alpha} + \vec{\epsilon}.
\end{align}$$

Where $$\vec{\epsilon}\sim \mathcal{N}(0, \sigma^2)$$. Splitting it up this way is pretty close to the [reparameterisation trick](https://en.wikipedia.org/wiki/Reparameterization_trick) where all the stochasticity has movied to $$\vec{\epsilon}$$ now and $$X \vec{\alpha}$$ is deterministic. But that's just a fun little aside.

As people who are aware of full probability theory, or want to go about this a lil rigorously, we would then want to maximise the probability of our parameters,

$$\begin{align}
\DeclareMathOperator*{\argmax}{arg\,max \;}
\argmax_\vec{\alpha} p(\vec{\alpha} \vert Y, X, \vec{\epsilon}) \propto \argmax_\vec{\alpha} {\color{red} p(Y, X, \vec{\epsilon} \vert \vec{\alpha})}{\color{blue} p(\vec{\alpha})}.
\end{align}$$

i.e. We want to find the _maximum a posteriori_ (MAP) estimate. As part of the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), the more parameters we include the higher our accuracy (less bias) but we get less individual information on the parameters (higher variance). Additionally, we run the risk of overfitting.

<div style="text-align: center;">
  <img 
      src="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/07/eba93f5a75070f0fbb9d86bec8a009e9.webp" 
      alt="Figure showing how the error behaves as model complexity grows, exemplifying the bias-variance trade off in statistics" 
      title="Figure showing how the error behaves as model complexity grows, exemplifying the bias-variance trade off in statistics" 
      style="width: 60%; height: auto; border-radius: 0px;">
      <figcaption>Figure showing how the error behaves as model complexity grows, exemplifying the bias-variance trade off in statistics. Source: https://cdn.analyticsvidhya.com/wp-content/uploads/2024/07/eba93f5a75070f0fbb9d86bec8a009e9.webp</figcaption>
</div>

The way that this is typically done is via regularisation on the parameters fitted, giving them a tendency to go towards 0, such that they only meaningfully contribute in the case where they really improve the fit. We can encode this as a prior on our parameters, that they are distributed according to a normal distribution with some variance $$\tau_j = \tau$$. (further assuming that we just put the same regularisation on all of them).

$$\begin{align}
\alpha_j \sim &\mathcal{N}(0, \tau_j^2)\\
&\textrm{or} \\
\vec{\alpha} \sim &\tau \mathcal{N}(0, \vec{1})
\end{align}$$

Where I denote a vector full of ones that is the relevant length $$\vec{1}$$, and when I need it the identity matrix as $$\mathbb{I}$$. We can thus write down our likelihood and prior, a value proportional to our posterior.

$$\begin{align}
\DeclareMathOperator*{\argmax}{arg\,max \;}
&\argmax_\vec{\alpha} p(\vec{\alpha} \vert Y, X, \vec{\epsilon}) \\
&\propto \argmax_\vec{\alpha} {\color{red}  \prod_i \frac{1}{\left(2\pi\right)^{1/2}} \det\left(C_Y \right)^{-1/2} \exp\left(-\frac{1}{2} (Y_i - X_i\vec{\alpha})^T C_X^{-1}(Y_i - X_i\vec{\alpha}) \right)} \\
&\;\;\;\;\;   \times {\color{blue} \frac{1}{\left(2\pi\right)^{k/2}} \det\left(C_\alpha \right)^{-1/2} \exp\left(-\frac{1}{2} \vec{\alpha}^T C_\alpha^{-1}\vec{\alpha} \right)}.
\end{align}$$

Where $$k$$ is the length/dimensionality of $$\vec{\alpha}$$, $$C_Y$$ is the covariance matrix for our data $$Y$$, which is just $$\sigma^2$$ and $$C_\alpha$$ is the covariance matrix for our parameters in our prior which is a diagonal matrix of size $$k$$ which $$\tau^2$$ on the diagonal. Simplifying the above, 

$$\begin{align}
\DeclareMathOperator*{\argmax}{arg\,max \;}
&\argmax_\vec{\alpha} p(\vec{\alpha} \vert Y, X, \vec{\epsilon}) \\
&\propto \argmax_\vec{\alpha} {\color{red} \exp\left(-\frac{1}{2\sigma^2} (Y - X\vec{\alpha})^2 \right)}{\color{blue} \exp\left(-\frac{1}{2\tau^2} \vec{\alpha}^2 \right)}.
\end{align}$$

Now because of numerical instability issues and many other reasons, we then take the log of this expression and reverse the sign.

$$\begin{align}
\DeclareMathOperator*{\argmin}{arg\,min \;}
&\argmin_\vec{\alpha} - \log p(\vec{\alpha} \vert Y, X, \vec{\epsilon}) \\
&= \argmin_\vec{\alpha} {\color{red}  \frac{1}{2\sigma^2} (Y - X\vec{\alpha})^2} {\color{blue} + \frac{1}{2\tau^2} \vec{\alpha}^2} + C.
\end{align}$$

Where $$C \in \mathbb{R} $$ is some arbitrary constant relating to the normalisation. If we're just wishing to minimise this expresion with respect to $$\vec{\alpha}$$ we can drop all the terms that don't involve it and multiply everything by the constant $$2\sigma^2$$ setting $$\sigma^2/\tau^2 = \lambda$$ to make the math nicer.

$$\begin{align}
L(\vec{\alpha}) = (Y - X\vec{\alpha})^2 + \lambda \vec{\alpha}^2
\end{align}$$

Which is just ordinary least squares with a $$L_2$$ regularisation on the parameters[^l2reg], or _ridge regression_. Note that the uncertainties only come through in how they influence the regularisation, which is because we assumed that they are shared the same noise variance. 

[^l2reg]: So called because it regularises the parameters using the $$L_2$$ or _euclidean_ norm.

What's the point of all this if we aren't going to use it though? Let's look at the relationship between the [Temperature and Ice Cream Sales](https://www.kaggle.com/datasets/raphaelmanayon/temperature-and-ice-cream-sales) Kaggle dataset.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/Ridge_Figures/IceCreamData.png" 
      alt="Temperature vs Ice cream sales data" 
      title="Temperature vs Ice cream sales data" 
      style="width: 70%; height: auto; border-radius: 0px;">
</div>

<br>

We can then see what the Ridge regression fit gives for different values of $$\lambda$$ remembering that $$\lambda=0$$ means no regularisation.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/Ridge_Figures/IceCreamData_with_fits_individual.png" 
      alt="Temperature vs Ice cream sales data and individual plots showing ridge fitting results" 
      title="Temperature vs Ice cream sales data and individual plots showing ridge fitting results" 
      style="width: 79%; height: auto; border-radius: 0px;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/Ridge_Figures/IceCreamData_with_fits.png" 
      alt="Temperature vs Ice cream sales data and all fitting results in a single plot" 
      title="Temperature vs Ice cream sales data and all fitting results in a single plot" 
      style="width: 79%; height: auto; border-radius: 0px;">
</div>

Here's the code.

```python
from scipy.optimize import minimize

linearY = data['Ice Cream Profits']
linearX = np.array([data['Temperature']*0 + 1., data['Temperature']]).T


def ridge_regression_cost(alpha, Y, X, lambdaval):
    return np.linalg.norm(Y - X @ alpha)**2 + lambdaval * np.linalg.norm(alpha)**2 


ridge_regression_cost(np.array([0., 2.]), linearY, linearX, 0.1)

result_dict = {}

for lambdaval in np.append(np.array([0.]), np.logspace(0, 3., 3)):
    linear_data_ridge_regression_result = minimize(
        ridge_regression_cost,
        x0 = np.array([0., 2.]),
        args = (linearY, linearX, lambdaval)
    )

    result_dict[lambdaval] = linear_data_ridge_regression_result.x
```

Alrighty, we have this very nice very automated way to perform our fitting. Let's look at another example of the path of a basketball.


<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/Ridge_Figures/Basketball_Data.png" 
      alt="Data showing the path of a basketball" 
      title="Data showing the path of a basketball" 
      style="width: 79%; height: auto; border-radius: 0px;">
</div>

<br>

We know beforehand that the path will be [parabolic](https://en.wikipedia.org/wiki/Parabolic_trajectory) and hence can't be modelled linearly (or at least properly). And now we can't use our very nice method any more ðŸ˜¢ ... or can we?

We employ _the exact same trick_ as in the previous two sections, particularly on $$x$$. We pretend that we have a new variable, that is really just $$x^2$$, and then fit a straight line in that space (everything is just dot or matrix products), which will be equivalent to fitting a polynomial, but to the framework, it's still just fitting a straight line! So now our data looks like.

$$\begin{align}
Y = \left[\begin{matrix}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{matrix} \right]
\end{align}$$

$$\begin{align}
X = \left[ 
\begin{matrix}
1 & x_1 & x_1^2\\
1 & x_2 & x_2^2\\
\vdots & \vdots \\
1 & x_N & x_N^2\\
\end{matrix}
\right]
\end{align}$$


$$\begin{align}
\vec{\alpha} = \left[\begin{matrix}
\alpha_0 & \alpha_1 & \alpha_2
\end{matrix}
\right]
\end{align}$$


Let's look at the data in the project space.

<iframe 
    src="/files/BlogPostData/2025-i-<3-kernels/Ridge_Figures/projected_basketball_data.html" 
    width="100%" 
    height="600px"
    title="Embedded Content"
    style="border:none;"
></iframe>

Wait, that still doesn't look linear? What's going on? Well that's because the problem is linear with respect to $$\vec{\alpha}$$ not $$X$$ still. The linear object we are fitting is closer to plane within which the curve we want is contained, and more specifically the region with the constraint that $$X_1 = X_2$$. Let's look at some results of the fitting (same code).


<div style="text-align: center; flex-wrap: wrap;">


<iframe 
    src="/files/BlogPostData/2025-i-<3-kernels/Ridge_Figures/projected_basketball_data_with_l0_fit.html" 
    width="69%" 
    height="600px"
    title="Embedded Content"
    style="border:none;"
></iframe>


<iframe 
    src="/files/BlogPostData/2025-i-<3-kernels/Ridge_Figures/projected_basketball_data_with_l1_fit.html" 
    width="69%" 
    height="600px"
    title="Embedded Content"
    style="border:none;"
></iframe>


<iframe 
    src="/files/BlogPostData/2025-i-<3-kernels/Ridge_Figures/projected_basketball_data_with_l2_fit.html" 
    width="69%" 
    height="600px"
    title="Embedded Content"
    style="border:none;"
></iframe>
</div>

<br>

We can also look at what these look like in the original space.

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/Ridge_Figures/Basketball_Data_With_Fits.png" 
      alt="Fitted baskbetball paths" 
      title="Fitted baskbetball paths" 
      style="width: 90%; height: auto; border-radius: 0px;">
</div>

<br>


We can see that as the regularisation tightens, or we increase $$\lambda$$, the constant term is particularly being penalised. That's great, but again, we were helped out by knowing what form of equation to expect. Additionally, what if something was periodic like this sunspot data?

<div style="text-align: center;">
  <img 
      src="/files/BlogPostData/2025-i-<3-kernels/Ridge_Figures/sunspot_data.png" 
      alt="Periodic sunspot data" 
      title="Periodic sunspot data" 
      style="width: 99%; height: auto; border-radius: 0px;">
</div>

<br>

Do we just give up or...?

## Principle Component Analysis (Kernel PCA I)

### Resources

- [Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis)
- [Kernel Principal component analysis - Wikipedia](https://en.wikipedia.org/wiki/Kernel_principal_component_analysis)
- [Principal Component Analysis (PCA) \| Dimensionality Reduction Techniques (2/5)](https://www.youtube.com/watch?v=ne6vnKoTHwk) - [DeepFindr](https://www.youtube.com/@DeepFindr)
- [19. Principal Component Analysis](https://www.youtube.com/watch?v=WW3ZJHPwvyg) - [MIT OCW](MIT OpenCourseWare)
- [Principle component regression - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_regression)
- [Principle component regression - Wikipedia](https://en.wikipedia.org/wiki/Principal_component_regression)
- [Higher Order Reduced Rank Regression - ArXiv: 2503.06528](https://arxiv.org/pdf/2503.06528)


### The Gist

To me Principle Component Analysis (PCA) is very similar to FLDA where we want to find a set of vectors that we can project the data on to best distinguish the data. PCA of course does it entirely differently (more robustly imo) but finding the direction(s) that maximise the variance in the given direction. The general process is as follows.

You have a set of n observations $$Z_i = [Z_i^1, Z_i^2, ..., Z_i^d] \in \mathbb{R}^d$$ with $$Z_i^j \in \mathbb{R}^1$$ with $$\mathbb{Z}^T = [Z_1, Z_2, ..., Z_n]$$.


<div style="text-align: center; flex-wrap: wrap;">
<img 
    src="/files/BlogPostData/2025-i-<3-kernels/PCA_Figures/PCA_data.gif" 
    alt="pca_data.png" 
    title="pca_data.png" 
    style="width: 49%; height: auto; border-radius: 0px;">
</div>


<br>


You shift it so that the mean is $$\vec{0}$$, $$X_i  = Z_i - \bar{Z}$$ or $$\mathbb{X}  = \mathbb{Z} - \frac{1}{n} \mathbb{Z} \vec{1}_n$$ where $$\vec{1}_n$$ is a vector of size $$n$$ with every element equal to $$1$$.



<div style="text-align: center; flex-wrap: wrap;">
<img 
    src="/files/BlogPostData/2025-i-<3-kernels/PCA_Figures/PCA_centralised_data.gif" 
    alt="centralised_data.png" 
    title="centralised_data.png" 
    style="width: 49%; height: auto; border-radius: 0px;">
</div>


<br>

We then calculate the covariance matrix of the transformed data by first noting that the empircal covariance, $$S$$, can be calculated as,

$$\begin{align}
S = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = \frac{1}{n}\sum_{i=1}^n X_i X_i^T = \frac{1}{n} \mathbb{X}^T\mathbb{X}.
\end{align}$$

We can then perform [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) which in this case is an [eigendecompoisition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) as $$S$$ is [positive semi-definite](https://en.wikipedia.org/wiki/Definite_matrix#Simultaneous_diagonalization) and thus [diagonalizable matrix](https://en.wikipedia.org/wiki/Diagonalizable_matrix). We know this as if we perform the operation for any vector in $$\mathbb{R}^d$$, then we get the variance in that direction. i.e.

$$\begin{align}
v^T S v = \textrm{variance in the direction of v} \geq 0
\end{align}$$

Hence, $$\exists P, D$$ where $$P$$ is orthonormal ($$P^T P = P P^T = \mathbb{I}$$) and $$D$$ is diagonal. Equivalently, this means we can represent $$P$$ and $$D$$ as,

$$\begin{align}
D = \left[ \begin{matrix}
\lambda_1 & 0 & \ldots & 0 \\
0 & \lambda_2 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & \lambda_d \\
\end{matrix}\right]
\end{align}$$

where $$\lambda_1 > \lambda_2 > ... >\lambda_d \geq 0$$ and

$$\begin{align}
P = \left[ \begin{matrix}
\vert & \vert &  & \vert \\
v_1 & v_2 & \ldots & v_d \\
\vert & \vert &  & \vert \\
\end{matrix}\right]
\end{align}$$

The $$P$$ matrix works as a projection operator that maps the centralised data into the space of eigenvectors, $$Y_i = P\, X_i$$, within which data's covariance matrix is the diagonal matrix $$D$$.

$$\begin{align}
S_Y &= \frac{1}{n}\sum_i Y_i \, Y_i^T \\
&= \frac{1}{n}\sum_i P\,X_i \,(P\,X_i)^T \\
&= \frac{1}{n}\sum_i P\,X_i \,X_i^T\,P^T \\
&= P\, \left(\frac{1}{n}\sum_i X_i \, X_i^T \right)\, P^T \\
&= P \,  P^T \, D \,P \, P^T \\
&= D
\end{align}$$

We can also show that the variance in direction along which the variance is maximised, $$\vec{\mu}$$ with $$\vert\vert\vec{\mu}\vert\vert = 1$$, have a magnitude less than the largest eigenvalue $$\lambda_1$$. 

$$\begin{align}
\vec{\mu}^T S \vec{\mu} &= \vec{\mu}^T \, P \, D \, P^T \, \vec{\mu} \\
&= (P^T \, \vec{\mu})^T \, D \, (P^T \, \vec{\mu}) \\
&= \vec{b}^T \, D \, \vec{b} \\
&= \sum_{j=1}^d \lambda_j b_j^2 \\
&\leq \lambda_1 \sum_{j=1}^d b_j^2 \\
&\leq \lambda_1 || \vec{b}||^2 \\
&\leq \lambda_1 || (P^T \, \vec{\mu})^T (P^T \, \vec{\mu}) || \\
&\leq \lambda_1 || \vec{\mu}^T \, P \, P^T \, \vec{\mu} || \\
&\leq \lambda_1 || \vec{\mu}^T \vec{\mu} || \\
&\leq \lambda_1 \\
\end{align}$$

And by the definition of the eigenvector and eigenvalues, but just to go through the math explicitly,

$$\begin{align}
\vec{v_1}^T S \vec{v_1} &= \vec{v_1}^T \, P \, D \, P^T \, \vec{v_1} \\
&= (P^T \, \vec{v_1})^T \, D \, (P^T \, \vec{v_1}) \\
&= \left(\left[ \begin{matrix}
- & \vec{v_1} &  & - \\
- & \vec{v_2} &  & - \\
- \\
- & \vec{v_d} &  & - \\
\end{matrix}\right] \vec{v_1} \right)^T \, D \, \left[ \begin{matrix}
- & \vec{v_1} &  & - \\
- & \vec{v_2} &  & - \\
 & - & \\
- & \vec{v_d} &  & - \\
\end{matrix}\right] \vec{v_1} \\
&= \left[ \begin{matrix}
1 & 0 & \ldots & 0
\end{matrix}\right] \, D \, \left[ \begin{matrix}
1 \\
0 \\
\vdots \\
0 \\
\end{matrix}\right] \\
&= \lambda_1 \\
\end{align}$$

Hence the eigenvectors are the ones that maximise the variance. So we skipped any optimisation (which wouldn't be easy) by some simple linear algebra with the projection of new points just involving some dot products with the orthonormal bases. Assuming we have some quick method to calculate the eigenvectors and eigenvalues our mission is done!

For our data, the covariance matrix is as follows.

$$\begin{align}
\begin{bmatrix} 
 12.687 & 17.438 & -9.877 \\
 17.438 & 38.263 & -18.308 \\
 -9.877 & -18.308 & 13.074 \\
\end{bmatrix}
\end{align}$$

Then just blindly using the [`np.linalg.eig`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html) function we find the eigenvalues to be,

$$\begin{align}
\begin{bmatrix} 
 56.956 \\
 4.111 \\
 2.957 \\
\end{bmatrix},
\end{align}$$

and eigenvectors to be

$$\begin{align}
\begin{bmatrix} 
 -0.412 & -0.691 & 0.594 \\
 -0.804 & 0.582 & 0.119 \\
 0.428 & 0.429 & 0.795 \\
\end{bmatrix}.
\end{align}$$

Let's chuck this in an interactive plot to see how the eigenvectors look against the data.

<div style="text-align: center; flex-wrap: wrap;">


<iframe 
    src="/files/BlogPostData/2025-i-<3-kernels/PCA_Figures/PCA_data_with_eigenvectors.html" 
    width="89%" 
    height="600px"
    title="Embedded Content"
    style="border:none;"
></iframe>

</div>

The process now gives us a whole set of vectors that we can use to reconstruct the data, but part of the point of this is to find the directions in the space of our data along which most of the information lies. So, we don't actually want to keep all the vectors, but how many do we decide to keep? Typically, people just keep 1 to 3 (easy to visualise, and should explain most of the variation) but if one want to do this a little more rigorously we can use something called a [Scree plot](https://en.wikipedia.org/wiki/Scree_plot). Apparently named after its resemblance to the natural formation of broken rocks at the bottom of a cliff (image below).


<div style="text-align: center; flex-wrap: wrap;">
<img 
    src="/files/BlogPostData/2025-i-<3-kernels/PCA_Figures/scree_nature.png" 
    alt="Picture of some natural scree" 
    title="Picture of some natural scree" 
    style="width: 69%; height: auto; border-radius: 0px;">
    <figcaption>Fig: Scree?</figcaption>
</div>

<br>

All that it is though is that you plot either the eigenvalues in order (automatically descending) where you look for an 'elbow' where the magnitude of the components drop off or their cumulative values $$\lambda_1$$, $$\lambda_1 + \lambda_2$$, $$\lambda_1 + \lambda_2 + \lambda_3$$, etc where instead you look for when the cumulation of the eigenvalues doesn't significantly increase (basically the same thing).

<div style="text-align: center; flex-wrap: wrap;">
<img 
    src="/files/BlogPostData/2025-i-<3-kernels/PCA_Figures/small_scree_plot.png" 
    alt="Picture of some unnatural scree" 
    title="Picture of some unnatural scree" 
    style="width: 49%; height: auto; border-radius: 0px;">
<img 
    src="/files/BlogPostData/2025-i-<3-kernels/PCA_Figures/small_cum_scree_plot.png" 
    alt="Picture of some unnatural cumulative scree" 
    title="Picture of some unnatural cumulative scree" 
    style="width: 49%; height: auto; border-radius: 0px;">
</div>

<br>

But let's say that the data has some periodic component? e.g. In the below (fake) data, if we blindly apply PCA the amplitude of the periodicity would be included in the variance. Despite the data haing relatively low noise compared to the baseline sinusoidal signal with linear trend. At this point I'll stop messing you around and just tell you the trick. (Unless you're one of the unfortunate few to look at this before it's finished.)

<div style="text-align: center; flex-wrap: wrap;">
<img 
    src="/files/BlogPostData/2025-i-<3-kernels/PCA_Figures/periodic_ice_cream_data.png" 
    alt="Fake Ice Cream Sales Data" 
    title="Fake Ice Cream Sales Data" 
    style="width: 79%; height: auto; border-radius: 0px;">
</div>

<br>


# The Kernel Trick

## Resources

- [The Kernel Trick](https://youtu.be/N_r9oJxSuRs) - [Udacity](https://www.youtube.com/@Udacity)
- [RBF kernel as an infinite feature expansion](https://andrewcharlesjones.github.io/journal/rbf.html) - [Andy Jones](https://andrewcharlesjones.github.io/)
- [Lecture 15 of 18 of Caltech's Machine Learning Course - CS 156](https://youtu.be/XUj5JbQihlU) - Professor Yaser Abu-Mostafa
- [Matthew N. Bernsteinâ€™s notes - The Radial Basis Function Kernel](https://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/svms/RBFKernel.pdf)
- [Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3)](https://www.youtube.com/watch?v=Qc5IyLW_hns) -[StatQuest with Josh Starmer](https://www.youtube.com/@statquest)


### The Gist

The idea that I keep coming back to in this post is that all of these methods on rely on just a series of dot products. I additionally showed that they all have (except PCA) much more generality and expressive power when we project our data into a higher dimensional space and use our linear methods there. But the grub was that even for low dimensional data, e.g. 4 for x, y, z and time, and just a degree 5 polynomial would have 59 columns in the design matrix (including constant). We then often perform an outer product so then we have to do $$59^2 = 3481$$ calculations .... instead of 16 .... real expensive.

For example let's say we have some 2D data,

$$\begin{align}
X^i = [x^i, y^i, z^i],
\end{align}$$

that I then project into the higher dimensional 'quadratic' feature space (with some slight scalar adjustments compared to previous sections that wouldn't make a difference to the end result), 

$$\begin{align}
X'^i = [1, {\color{red} \sqrt{2} x^i}, {\color{blue}\sqrt{2} y^i }, {\color{green}\sqrt{2} z^i}, {\color{purple} (x^i)^2}, {\color{orange} \sqrt{2} (x^i) \, (y^i)}, {\color{magenta} (y^i)^2 }, {\color{teal} \sqrt{2} (z^i)\, (y^i) }, {\color{violet} (z^i)^2 }].
\end{align}$$

We've also established that all of these methods simply compute the dot products between the vectors in this transformed space. 


$$\begin{align}
(X'^i)^T (X'^j) = 1 &+ {\color{red} 2 x^i \,x^j} + {\color{blue} 2 y^i \, y^j} + {\color{green} 2 z^i\, z^j }\\ 
&+ {\color{purple} (x^i)^2 \, (x^j)^2 } +{\color{orange} 2 (x^i) \, (y^i) \, (x^j) \, (y^j) } \\
&+ {\color{magenta}  (y^i)^2 \, (y^j)^2 } + {\color{teal} 2 (z^i)\, (y^i) \, (z^j)\, (y^j) } +  {\color{violet}  (z^i)^2 \, (z^j)^2}
\end{align}$$

Then the magical thing is that this is equivalent to,

$$\begin{align}
(X'^i)^T (X'^j) = (1 + (X^i)^T (X^j))^2 = K^{\textrm{poly}}_2(X^i, X^j),
\end{align}$$

where $$K^{\textrm{poly}}_2(X^i, X^j)$$ is referred to as a ___kernel function___, more specifically a __degree 2 polynomial kernel__. Similarly we can look at two dimensional data,

$$\begin{align}
X^i = [x^i, y^i],
\end{align}$$

and transform it into the cubic space,

$$\begin{align}
X'^i = [1, {\color{red} \sqrt{3} x^i}, {\color{blue}\sqrt{3} y^i }, {\color{purple} \sqrt{3} (x^i)^2}, {\color{orange} \sqrt{6} (x^i) \, (y^i)}, {\color{magenta}\sqrt{3} (y^i)^2 }, {\color{teal} (x^i)^3 }, {\color{brown} \sqrt{3} (x^i)^2 (y^i) },{\color{Tan}  \sqrt{3} (x^i) (y^i)^2}, {\color{violet} (y^i)^3  }],
\end{align}$$

the dot product between any two points in the this feature space would be,


$$\begin{align}
(X'^i)^T(X'^j) &= 1 + {\color{red} 3 x^i \, x^j} + {\color{blue} 3 y^i \, y^j } + {\color{purple}3 (x^i)^2 \, (x^j)^2} \\
&\;\;\;\;\;\;\;\, + {\color{orange} 6 (x^i y^i) \, (x^j y^j)} \;\; + {\color{magenta} 3 (y^i)^2 \, (y^j)^2 } + {\color{teal} (x^i)^3 \, (x^j)^3 } \\
&\;\;\;\;\;\;\;\, + {\color{brown} 3 ((x^i)^2 (y^i) )\, ((x^j)^2 (y^j) )} \;\;\;\;\;\;\;\;\;\, + {\color{Tan}  3 ((x^i) (y^i)^2) \, ((x^j) (y^j)^2) } \\
&\;\;\;\;\;\;\;\, + {\color{violet} (y^i)^3 (y^j)^3  } \\
&= (1 + (X^i)^T (X^j))^3 \\
&= K^{\textrm{poly}}_3(X^i, X^j) \\
\end{align}$$

For the second last line I'm just going to have to ask you to trust me if it doesn't immediately seem right...

So the key point is that we can effectively do the calculations in the higher dimensional space without actually having to explicitly first transform our data into that space, as long as our kernel function is a valid inner product.
What we can do in practice is store the values of the inner products between our data in what's called a _Kernel_ or _Gram_ matrix. For now, we can just think of how the kernel's behave in the context of the variables we give them.
So we'll plot the examples of the matrix for some 1D variable and just examine how the kernel behaves as a function of space.

For the degree 1 polynomial (directly below) we can see that the relationship between $$x^i$$ and $$x^j$$ is linear, as expected as we're using a linear kernel. 
For a given $$x^j$$ the value decreases as you move to the left and increases as you move to the right.
<div style="text-align: center; flex-wrap: wrap;">
<img 
    src="/files/BlogPostData/2025-i-<3-kernels/KernelTrick_Figures/degree1poly_kernel_matrix.png" 
    alt="Gram matrix for 1 degree polynomial kernel" 
    title="Gram matrix for 1 degree polynomial kernel" 
    style="width: 89%; height: auto; border-radius: 0px;">
</div>

<br>
 
For the degree 2 polynomial (directly below) we can see that the relationship between $$x^i$$ and $$x^j$$ is quadratic. For a single value of $$x^j$$ you can see that multiple values of $$x^i$$ give the same inner product value. 
Meaning that if $$x^j$$ was equal to 1 for example, $$x^i=1$$ and $$x^i=-1$$ would be equivalent from the eyes of the inner product.

<div style="text-align: center; flex-wrap: wrap;">
<img 
    src="/files/BlogPostData/2025-i-<3-kernels/KernelTrick_Figures/degree2poly_kernel_matrix.png" 
    alt="Gram matrix for 2 degree polynomial kernel" 
    title="Gram matrix for 2 degree polynomial kernel" 
    style="width: 89%; height: auto; border-radius: 0px;">
</div>

<br>
 
And for the degree 3 polynomial (directly below) we can see that the relationship between $$x^i$$ and $$x^j$$ is cubic. 
And we can see more nonlinear behaviour pop up but the functions are monotonic so multiple values don't give the same answer (for 1D, isn't true for higher).

<div style="text-align: center; flex-wrap: wrap;">
<img 
    src="/files/BlogPostData/2025-i-<3-kernels/KernelTrick_Figures/degree3poly_kernel_matrix.png" 
    alt="Gram matrix for 3 degree polynomial kernel" 
    title="Gram matrix for 3 degree polynomial kernel" 
    style="width: 89%; height: auto; border-radius: 0px;">
</div>

<br>


Not only can we implicitly transform our data into some finite dimensional polynomial space, we can also use kernels where the number of terms would be technically infinite.

e.g. the most commonly used kernel across disciplines is the [Radial basis function kernel](https://en.wikipedia.org/wiki/Radial_basis_function_kernel) or simply the RBF kernel, defined as,

$$\begin{align}
K(X_i, X_j) = \exp\left(- \frac{\vert\vert X_i - X_j\vert\vert^2}{2\sigma^2} \right),
\end{align}$$

seeing as it has an exponential in it you can see that if we made the equivalent expansion as above that the series would be infinite. Setting the _length scale_ $$\sigma$$ to $$1$$.

$$\begin{align}
K(X_i, X_j) &= \exp\left(- \frac{\vert\vert X_i - X_j\vert\vert^2}{2} \right) \\
&= \exp\left[-\frac{1}{2} \left( (X^i)^2  + (X^j)^2\right)\right] \sum_{n=0}^\infty \frac{K^{poly}_n(X^i, X^j)}{n!} \\
&= \sum_{n=0}^\infty \frac{K^{poly}_n(X^i, X^j)}{n!} \\
& \;\;\;\;\;\;\; \div \left(\sum_{m=0}^\infty \frac{(\frac{1}{2} (X^i)^T (X^i))^m}{m!}\right)\\
& \;\;\;\;\;\;\; \div \left(\sum_{m'=0}^\infty \frac{(\frac{1}{2} (X^j)^T (X^j))^{m'}}{(m')!}\right)
\end{align}$$

What the inner products between vectors would imply is pretty much a gaussian distribution with euclidean distance metric, or more simply, a quantity related to how close they are (that follows a gaussian distribution) or something like a weighted nearest neighbour model. 
If they're far away from each other, the inner product is small, if they are near each other, the inner product is large. Spread out in a smooth manner due to the gaussian distribution.

Although the RBF Kernel works in infinite dimensions, we can visualise it's truncated version to see how the behaviour arises. 
Thanks to [Andy Jones](https://andrewcharlesjones.github.io/) for the heatmap code in their blog posts [RBF kernel as an infinite feature expansion](https://andrewcharlesjones.github.io/journal/rbf.html) so I didn't have to write it myself, please check it out if you have the time!



<div style="text-align: center; flex-wrap: wrap;">
<img 
    src="/files/BlogPostData/2025-i-<3-kernels/KernelTrick_Figures/finite_rbf_2.png" 
    alt="Gram matrix for truncated RBF kernel for q=m=m'=2" 
    title="Gram matrix for truncated RBF kernel for q=m=m'=2" 
    style="width: 49%; height: auto; border-radius: 0px;">


<img 
    src="/files/BlogPostData/2025-i-<3-kernels/KernelTrick_Figures/finite_rbf_3.png" 
    alt="Gram matrix for truncated RBF kernel for q=m=m'=3" 
    title="Gram matrix for truncated RBF kernel for q=m=m'=3" 
    style="width: 49%; height: auto; border-radius: 0px;">



<img 
    src="/files/BlogPostData/2025-i-<3-kernels/KernelTrick_Figures/finite_rbf_4.png" 
    alt="Gram matrix for truncated RBF kernel for q=m=m'=4" 
    title="Gram matrix for truncated RBF kernel for q=m=m'=4" 
    style="width: 49%; height: auto; border-radius: 0px;">



<img 
    src="/files/BlogPostData/2025-i-<3-kernels/KernelTrick_Figures/finite_rbf_5.png" 
    alt="Gram matrix for truncated RBF kernel for q=m=m'=5" 
    title="Gram matrix for truncated RBF kernel for q=m=m'=5" 
    style="width: 49%; height: auto; border-radius: 0px;">



<img 
    src="/files/BlogPostData/2025-i-<3-kernels/KernelTrick_Figures/finite_rbf_10.png" 
    alt="Gram matrix for truncated RBF kernel for q=m=m'=10" 
    title="Gram matrix for truncated RBF kernel for q=m=m'=10" 
    style="width: 49%; height: auto; border-radius: 0px;">



<img 
    src="/files/BlogPostData/2025-i-<3-kernels/KernelTrick_Figures/finite_rbf_100.png" 
    alt="Gram matrix for truncated RBF kernel for q=m=m'=100" 
    title="Gram matrix for truncated RBF kernel for q=m=m'=100" 
    style="width: 49%; height: auto; border-radius: 0px;">


<img 
    src="/files/BlogPostData/2025-i-<3-kernels/KernelTrick_Figures/RBF_kernel_matrix.png" 
    alt="Gram matrix for RBF kernel" 
    title="Gram matrix for RBF kernel" 
    style="width: 99%; height: auto; border-radius: 0px;">
</div>

<br>

Essentially as stated above, the inner product/kernel function value is large when the variables are of a similar value. 

There are a bunch more kernels to model different behaviours including the [periodic kernel](https://www.cs.toronto.edu/~duvenaud/cookbook/#:~:text=of%20model%20misspecification.-,Periodic%20Kernel,-k)
and [Matern kernel](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html), and you can combine them to model different behaviours! You can play around with some different
kernels [here](https://www.infinitecuriosity.org/vizgp/). It's a Gaussian Process interactive website that you can just think of it as something that will plot the forms of equations/relationships between coordinates that the kernels imply.
I'll demonstrate some more through the following examples.


# Awesome Kernel-Based Methods

## Kernel Discriminatory Analysis (KDA II)

### Resources

- [Kernel Fisher discriminant analysis](https://en.wikipedia.org/wiki/Kernel_Fisher_discriminant_analysis)
- [Manifold hypothesis](https://en.wikipedia.org/wiki/Manifold_hypothesis)

### The Gist



## Support Vector Machines (SVM II)

### Resources

- [Manifold hypothesis](https://en.wikipedia.org/wiki/Manifold_hypothesis)

### The Gist




## Kernel Ridge Regression (KRR II)

### Resources



### The Gist




## Kernel Principle Component Analysis (Kernel PCA II)

### Resources

- [Manifold hypothesis](https://en.wikipedia.org/wiki/Manifold_hypothesis)


### The Gist








# Gallery of Kernels

## Resources








# Gaussian Processes

## Resources

- [A Practical Guide to Gaussian Processes](https://infallible-thompson-49de36.netlify.app/)
- [Interactive Gaussian Process Visualization](https://www.infinitecuriosity.org/vizgp/)
- [Gaussian process regression demo](https://www.tmpl.fi/gp/)
- [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/chapters/RW.pdf)
- [2023-01-09 PRML - From Bayesian Linear Regression to Gaussian processes](https://www.youtube.com/watch?v=148EUutsU8Q) - Stefan Harmeling


### The Gist














# Conclusions / Pros and Cons of Kernel Methods









